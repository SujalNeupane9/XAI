{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6909860,"sourceType":"datasetVersion","datasetId":3968241},{"sourceId":11003597,"sourceType":"datasetVersion","datasetId":6849972},{"sourceId":272003408,"sourceType":"kernelVersion"},{"sourceId":280550089,"sourceType":"kernelVersion"},{"sourceId":281119483,"sourceType":"kernelVersion"},{"sourceId":282200743,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv('/kaggle/input/creating-additional-features-for-pan-clef/train.csv')\ntest_df = pd.read_csv('/kaggle/input/creating-additional-features-for-pan-clef/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:29.662992Z","iopub.execute_input":"2025-12-04T12:50:29.663617Z","iopub.status.idle":"2025-12-04T12:50:34.477416Z","shell.execute_reply.started":"2025-12-04T12:50:29.663586Z","shell.execute_reply":"2025-12-04T12:50:34.476588Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.478559Z","iopub.execute_input":"2025-12-04T12:50:34.478777Z","iopub.status.idle":"2025-12-04T12:50:34.511094Z","shell.execute_reply.started":"2025-12-04T12:50:34.478759Z","shell.execute_reply":"2025-12-04T12:50:34.510433Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n0  ea468d03-1973-5039-86b2-ff225bb92c4e   \n1  0d05f269-6d67-521d-9b5d-cc18f482c6c1   \n2  c2ec79f3-da80-58f8-bef0-3e0ea7ab072f   \n3  4ad37c58-0bb7-536b-997d-cfccabd0d094   \n4  07747b0c-5051-5e0d-8096-b4d4ed8bd98e   \n\n                                                text  \\\n0  Duke Ellington, a titan of jazz, revolutionize...   \n1  I reflected on the shifting dynamics of media ...   \n2  In F. Scott Fitzgerald's \"The Great Gatsby,\" t...   \n3  I still chuckle when I think about that time I...   \n4  Yoga, originating in ancient India, encompasse...   \n\n                          model  label   genre  character_count  word_count  \\\n0          falcon3-10b-instruct      1  essays             2480         362   \n1                       o3-mini      1  essays             3395         515   \n2                        gpt-4o      1  essays             3288         494   \n3  deepseek-r1-distill-qwen-32b      1  essays             2347         432   \n4              gemini-2.0-flash      1  essays             3588         520   \n\n   sentence_count  paragraph_count  stopword_count  ...  specificity_score  \\\n0              13                7             138  ...           0.316901   \n1              21                7             208  ...           0.268421   \n2              21               11             199  ...           0.286458   \n3              25                7             204  ...           0.206226   \n4              25                9             183  ...           0.327532   \n\n  figurative_language_score  paragraph_coherence_consistency  \\\n0                         4                         0.268389   \n1                         2                         0.312356   \n2                         4                         0.278783   \n3                         8                         0.229710   \n4                         1                         0.361226   \n\n   predictability_score  hedge_uncertainty_score  transition_variety_score  \\\n0              5.073228                        0                         1   \n1              5.291934                        0                         0   \n2              5.122549                        1                         1   \n3              5.105177                        1                         0   \n4              5.126124                        0                         1   \n\n   grammatical_mistakes  pos_2gram_variety  pos_3gram_variety  \\\n0                     3                 67                161   \n1                     3                 85                214   \n2                     1                 89                220   \n3                     0                 86                203   \n4                    22                 74                190   \n\n   pos_4gram_variety  \n0                254  \n1                358  \n2                347  \n3                318  \n4                338  \n\n[5 rows x 44 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>model</th>\n      <th>label</th>\n      <th>genre</th>\n      <th>character_count</th>\n      <th>word_count</th>\n      <th>sentence_count</th>\n      <th>paragraph_count</th>\n      <th>stopword_count</th>\n      <th>...</th>\n      <th>specificity_score</th>\n      <th>figurative_language_score</th>\n      <th>paragraph_coherence_consistency</th>\n      <th>predictability_score</th>\n      <th>hedge_uncertainty_score</th>\n      <th>transition_variety_score</th>\n      <th>grammatical_mistakes</th>\n      <th>pos_2gram_variety</th>\n      <th>pos_3gram_variety</th>\n      <th>pos_4gram_variety</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ea468d03-1973-5039-86b2-ff225bb92c4e</td>\n      <td>Duke Ellington, a titan of jazz, revolutionize...</td>\n      <td>falcon3-10b-instruct</td>\n      <td>1</td>\n      <td>essays</td>\n      <td>2480</td>\n      <td>362</td>\n      <td>13</td>\n      <td>7</td>\n      <td>138</td>\n      <td>...</td>\n      <td>0.316901</td>\n      <td>4</td>\n      <td>0.268389</td>\n      <td>5.073228</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>67</td>\n      <td>161</td>\n      <td>254</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0d05f269-6d67-521d-9b5d-cc18f482c6c1</td>\n      <td>I reflected on the shifting dynamics of media ...</td>\n      <td>o3-mini</td>\n      <td>1</td>\n      <td>essays</td>\n      <td>3395</td>\n      <td>515</td>\n      <td>21</td>\n      <td>7</td>\n      <td>208</td>\n      <td>...</td>\n      <td>0.268421</td>\n      <td>2</td>\n      <td>0.312356</td>\n      <td>5.291934</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>85</td>\n      <td>214</td>\n      <td>358</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c2ec79f3-da80-58f8-bef0-3e0ea7ab072f</td>\n      <td>In F. Scott Fitzgerald's \"The Great Gatsby,\" t...</td>\n      <td>gpt-4o</td>\n      <td>1</td>\n      <td>essays</td>\n      <td>3288</td>\n      <td>494</td>\n      <td>21</td>\n      <td>11</td>\n      <td>199</td>\n      <td>...</td>\n      <td>0.286458</td>\n      <td>4</td>\n      <td>0.278783</td>\n      <td>5.122549</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>89</td>\n      <td>220</td>\n      <td>347</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4ad37c58-0bb7-536b-997d-cfccabd0d094</td>\n      <td>I still chuckle when I think about that time I...</td>\n      <td>deepseek-r1-distill-qwen-32b</td>\n      <td>1</td>\n      <td>essays</td>\n      <td>2347</td>\n      <td>432</td>\n      <td>25</td>\n      <td>7</td>\n      <td>204</td>\n      <td>...</td>\n      <td>0.206226</td>\n      <td>8</td>\n      <td>0.229710</td>\n      <td>5.105177</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86</td>\n      <td>203</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>07747b0c-5051-5e0d-8096-b4d4ed8bd98e</td>\n      <td>Yoga, originating in ancient India, encompasse...</td>\n      <td>gemini-2.0-flash</td>\n      <td>1</td>\n      <td>essays</td>\n      <td>3588</td>\n      <td>520</td>\n      <td>25</td>\n      <td>9</td>\n      <td>183</td>\n      <td>...</td>\n      <td>0.327532</td>\n      <td>1</td>\n      <td>0.361226</td>\n      <td>5.126124</td>\n      <td>0</td>\n      <td>1</td>\n      <td>22</td>\n      <td>74</td>\n      <td>190</td>\n      <td>338</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 44 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.511846Z","iopub.execute_input":"2025-12-04T12:50:34.512080Z","iopub.status.idle":"2025-12-04T12:50:34.518979Z","shell.execute_reply.started":"2025-12-04T12:50:34.512059Z","shell.execute_reply":"2025-12-04T12:50:34.518253Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'text', 'model', 'label', 'genre', 'character_count',\n       'word_count', 'sentence_count', 'paragraph_count', 'stopword_count',\n       'unique_word_count', 'pos_counts', 'sentiment_polarity',\n       'sentiment_subjectivity', 'discourse_marker_count', 'vocab_size',\n       'sentence_complexity', 'punctuation_count',\n       'sentence_length_difference', 'type_token_ratio', 'word_entropy',\n       'flesch_reading_ease', 'gzip_ratio', 'negation_freq',\n       'question_stmt_ratio', 'clause_sentence_ratio', 'modal_freq',\n       'pronoun_ratio', 'pos_diversity', 'hapax_ratio',\n       'sentence_length_variation', 'repetition_rate', 'personal_voice_score',\n       'emotion_variation', 'specificity_score', 'figurative_language_score',\n       'paragraph_coherence_consistency', 'predictability_score',\n       'hedge_uncertainty_score', 'transition_variety_score',\n       'grammatical_mistakes', 'pos_2gram_variety', 'pos_3gram_variety',\n       'pos_4gram_variety'],\n      dtype='object')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"FEATURES = ['character_count', 'word_count', 'paragraph_count', 'stopword_count',\n       'unique_word_count', 'sentiment_subjectivity', 'discourse_marker_count',\n       'sentence_complexity', 'punctuation_count',\n       'sentence_length_difference', 'type_token_ratio', 'word_entropy',\n       'flesch_reading_ease', 'gzip_ratio', 'question_stmt_ratio',\n       'clause_sentence_ratio', 'modal_freq', 'pronoun_ratio', 'pos_diversity',\n       'hapax_ratio', 'sentence_length_variation', 'repetition_rate',\n       'specificity_score', 'figurative_language_score',\n       'paragraph_coherence_consistency', 'transition_variety_score',\n       'grammatical_mistakes', 'pos_2gram_variety', 'pos_3gram_variety',\n       'pos_4gram_variety'\n]\nTARGET = 'label'\n\nprint(len(FEATURES))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.520417Z","iopub.execute_input":"2025-12-04T12:50:34.520692Z","iopub.status.idle":"2025-12-04T12:50:34.536094Z","shell.execute_reply.started":"2025-12-04T12:50:34.520663Z","shell.execute_reply":"2025-12-04T12:50:34.535418Z"}},"outputs":[{"name":"stdout","text":"30\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 'character_count', 'word_count', 'sentence_count', 'paragraph_count',\n#        'stopword_count', 'unique_word_count', 'sentiment_subjectivity',\n#        'vocab_size', 'punctuation_count', 'sentence_length_difference',\n#        'type_token_ratio', 'word_entropy', 'flesch_reading_ease', 'gzip_ratio',\n#        'negation_freq', 'question_stmt_ratio', 'clause_sentence_ratio',\n#        'pronoun_ratio', 'hapax_ratio', 'sentence_length_variation',\n#        'repetition_rate', 'personal_voice_score', 'specificity_score',\n#        'paragraph_coherence_consistency', 'predictability_score',\n#        'hedge_uncertainty_score', 'grammatical_mistakes', 'pos_2gram_variety',\n#        'pos_3gram_variety', 'pos_4gram_variety'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.536848Z","iopub.execute_input":"2025-12-04T12:50:34.537081Z","iopub.status.idle":"2025-12-04T12:50:34.549866Z","shell.execute_reply.started":"2025-12-04T12:50:34.537061Z","shell.execute_reply":"2025-12-04T12:50:34.549036Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"X_train = train_df[FEATURES]\ny_train = train_df[TARGET]\n\nX_test = test_df[FEATURES]\ny_test = test_df[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.550586Z","iopub.execute_input":"2025-12-04T12:50:34.550968Z","iopub.status.idle":"2025-12-04T12:50:34.570554Z","shell.execute_reply.started":"2025-12-04T12:50:34.550946Z","shell.execute_reply":"2025-12-04T12:50:34.569866Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"X_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.571272Z","iopub.execute_input":"2025-12-04T12:50:34.571528Z","iopub.status.idle":"2025-12-04T12:50:34.593055Z","shell.execute_reply.started":"2025-12-04T12:50:34.571499Z","shell.execute_reply":"2025-12-04T12:50:34.592403Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 23707 entries, 0 to 23706\nData columns (total 30 columns):\n #   Column                           Non-Null Count  Dtype  \n---  ------                           --------------  -----  \n 0   character_count                  23707 non-null  int64  \n 1   word_count                       23707 non-null  int64  \n 2   paragraph_count                  23707 non-null  int64  \n 3   stopword_count                   23707 non-null  int64  \n 4   unique_word_count                23707 non-null  int64  \n 5   sentiment_subjectivity           23707 non-null  float64\n 6   discourse_marker_count           23707 non-null  int64  \n 7   sentence_complexity              23707 non-null  float64\n 8   punctuation_count                23707 non-null  int64  \n 9   sentence_length_difference       23707 non-null  int64  \n 10  type_token_ratio                 23707 non-null  float64\n 11  word_entropy                     23707 non-null  float64\n 12  flesch_reading_ease              23707 non-null  float64\n 13  gzip_ratio                       23707 non-null  float64\n 14  question_stmt_ratio              23707 non-null  float64\n 15  clause_sentence_ratio            23707 non-null  float64\n 16  modal_freq                       23707 non-null  float64\n 17  pronoun_ratio                    23707 non-null  float64\n 18  pos_diversity                    23707 non-null  float64\n 19  hapax_ratio                      23707 non-null  float64\n 20  sentence_length_variation        23707 non-null  float64\n 21  repetition_rate                  23707 non-null  float64\n 22  specificity_score                23707 non-null  float64\n 23  figurative_language_score        23707 non-null  int64  \n 24  paragraph_coherence_consistency  23707 non-null  float64\n 25  transition_variety_score         23707 non-null  int64  \n 26  grammatical_mistakes             23707 non-null  int64  \n 27  pos_2gram_variety                23707 non-null  int64  \n 28  pos_3gram_variety                23707 non-null  int64  \n 29  pos_4gram_variety                23707 non-null  int64  \ndtypes: float64(16), int64(14)\nmemory usage: 5.4 MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from xgboost import XGBClassifier\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport re\nimport joblib\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:34.593818Z","iopub.execute_input":"2025-12-04T12:50:34.594067Z","iopub.status.idle":"2025-12-04T12:50:57.261172Z","shell.execute_reply.started":"2025-12-04T12:50:34.594043Z","shell.execute_reply":"2025-12-04T12:50:57.260520Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom collections import Counter\nimport spacy\nimport re\nfrom textblob import Word\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\nimport math\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef sentence_length_variation(text):\n    sentences = sent_tokenize(text)\n    lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n\n    if len(lengths) < 2:\n        return 0.0  \n\n    return np.std(lengths)   # Standard deviation\n\ndef repetition_rate(text):\n    words = [w.lower() for w in text.split()]\n    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n    if len(bigrams) == 0:\n        return 0\n\n    counts = Counter(bigrams)\n    repeated = sum(1 for bg, c in counts.items() if c > 1)\n\n    return repeated / len(bigrams)\n\ndef personal_voice_score(text):\n    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n    words = [w.lower() for w in text.split()]\n    count = sum(1 for w in words if w in personal_pronouns)\n    if len(words) == 0:\n        return 0\n    return count / len(words)\n\ndef emotion_variation(text):\n    sentences = sent_tokenize(text)\n    if len(sentences) < 2:\n        return 0\n\n    sentiments = [TextBlob(s).sentiment.polarity for s in sentences]\n    diffs = [abs(sentiments[i] - sentiments[i+1]) for i in range(len(sentiments)-1)]\n\n    return np.mean(diffs)\n\n\ndef specificity_score(text):\n    doc = nlp(text)\n    concrete_tags = {\"NOUN\", \"PROPN\", \"NUM\"}  \n    concrete_count = sum(1 for token in doc if token.pos_ in concrete_tags)\n    if len(doc) == 0:\n        return 0\n    return concrete_count / len(doc)\n\n\ndef imperfection_score(text):\n    words = [w for w in re.findall(r\"\\b\\w+\\b\", text)]\n    if len(words) == 0:\n        return 0\n\n    misspelled = sum(1 for w in words if Word(w).correct().lower() != w.lower())\n    return misspelled / len(words)\n\nfigurative_markers = [\n    \"like\", \"as if\", \"as though\", \"metaphor\", \"symbolic\", \n    \"resembles\", \"reminds me of\", \"figurative\"\n]\n\ndef figurative_language_score(text):\n    t = text.lower()\n    count = sum(t.count(m) for m in figurative_markers)\n    return count\n\ndef paragraph_coherence_consistency(text):\n    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 0]\n\n    if len(paragraphs) < 2:\n        return 0\n\n    vec = TfidfVectorizer().fit_transform(paragraphs)\n    sims = []\n\n    for i in range(len(paragraphs)-1):\n        sim = cosine_similarity(vec[i], vec[i+1])[0][0]\n        sims.append(sim)\n\n    return np.mean(sims)\n\n\ndef predictability_score(text):\n    words = [w.lower() for w in text.split()]\n    counts = Counter(words)\n    total = len(words)\n    if total == 0:\n        return 0\n\n    probs = [counts[w]/total for w in words]\n    surprise = [-math.log(p) for p in probs]\n\n    return np.mean(surprise)\n\nhedge_words = {\n    \"maybe\", \"perhaps\", \"sort of\", \"kind of\", \"i guess\", \n    \"probably\", \"possibly\", \"apparently\", \"roughly\"\n}\n\ndef hedge_uncertainty_score(text):\n    t = text.lower()\n    count = sum(t.count(hw) for hw in hedge_words)\n    return count\n\ntransitions = [\n    \"however\", \"therefore\", \"meanwhile\", \"moreover\", \"furthermore\",\n    \"in contrast\", \"on the other hand\", \"overall\", \"in summary\"\n]\n\ndef transition_variety_score(text):\n    t = text.lower()\n    count = sum(t.count(word) for word in transitions)\n    return count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:50:57.261895Z","iopub.execute_input":"2025-12-04T12:50:57.262372Z","iopub.status.idle":"2025-12-04T12:51:06.115503Z","shell.execute_reply.started":"2025-12-04T12:50:57.262345Z","shell.execute_reply":"2025-12-04T12:51:06.114565Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Define models to train\nmodels = {\n    'XGB': XGBClassifier(eval_metric='logloss', random_state=47),\n    'Random Forest': RandomForestClassifier(random_state=47),\n    'Logistic Regression': Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', LogisticRegression(random_state=47))\n    ]),\n    'SVM': Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', SVC(probability=True, random_state=47))\n    ]),    \n}\n\nresults = []\nPAN_preds = {}\n\nfor model_name, model in models.items():\n    print(f\"Training {model_name}...\")\n    \n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Predict on test data\n    y_pred = model.predict(X_test)\n    PAN_preds[model_name] = y_pred\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    # Save the model using joblib\n    joblib.dump(model, f\"{model_name.replace(' ', '_')}_model.joblib\")\n\n    results.append({\n        'Model': model_name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:06.118678Z","iopub.execute_input":"2025-12-04T12:51:06.119278Z","iopub.status.idle":"2025-12-04T12:51:47.582137Z","shell.execute_reply.started":"2025-12-04T12:51:06.119256Z","shell.execute_reply":"2025-12-04T12:51:47.581415Z"}},"outputs":[{"name":"stdout","text":"Training XGB...\nTraining Random Forest...\nTraining Logistic Regression...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Training SVM...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# # Convert to numpy array for easier manipulation\n# preds_array = np.array(preds)\n\n# # Get majority prediction - axis=0 calculates mode across models for each sample\n# majority_result = mode(preds_array, axis=0)\n\n# # Extract just the majority predictions (ignore the counts)\n# majority_predictions = majority_result.mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.582884Z","iopub.execute_input":"2025-12-04T12:51:47.583075Z","iopub.status.idle":"2025-12-04T12:51:47.587861Z","shell.execute_reply.started":"2025-12-04T12:51:47.583059Z","shell.execute_reply":"2025-12-04T12:51:47.587034Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# print(f\"The f1 score after majority voting is {f1_score(y_test, majority_predictions)}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.588684Z","iopub.execute_input":"2025-12-04T12:51:47.588862Z","iopub.status.idle":"2025-12-04T12:51:47.604365Z","shell.execute_reply.started":"2025-12-04T12:51:47.588847Z","shell.execute_reply":"2025-12-04T12:51:47.603684Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"results_df = pd.DataFrame(results)\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.605197Z","iopub.execute_input":"2025-12-04T12:51:47.605485Z","iopub.status.idle":"2025-12-04T12:51:47.620819Z","shell.execute_reply.started":"2025-12-04T12:51:47.605445Z","shell.execute_reply":"2025-12-04T12:51:47.620096Z"}},"outputs":[{"name":"stdout","text":"                 Model  Accuracy  Precision    Recall  F1 Score\n0                  XGB  0.967400   0.970424  0.979239  0.974812\n1        Random Forest  0.950404   0.947943  0.976644  0.962079\n2  Logistic Regression  0.940373   0.941498  0.967561  0.954352\n3                  SVM  0.965450   0.967122  0.979671  0.973356\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nplt.switch_backend('agg')\n\n# Get predictions\npan_xgb_preds = models['XGB'].predict(X_test)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, pan_xgb_preds, labels=[0,1])\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Human (0)', 'AI (1)'])\nfig, ax = plt.subplots(figsize=(3, 3))\ndisp.plot(cmap='Blues', ax=ax)\nplt.title(\"Confusion Matrix: XGB on PAN CLEF Dataset\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.621809Z","iopub.execute_input":"2025-12-04T12:51:47.622025Z","iopub.status.idle":"2025-12-04T12:51:47.746552Z","shell.execute_reply.started":"2025-12-04T12:51:47.622008Z","shell.execute_reply":"2025-12-04T12:51:47.745903Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.dpi\"] = 160      \nfig, ax = plt.subplots(figsize=(15, 12))\n\nxgb.plot_importance(\n    models['XGB'],\n    max_num_features=100,\n    importance_type=\"gain\",\n    ax=ax,\n    show_values=False,                \n    grid=False\n)\n\nax.set_title(\"XGB Feature Importances\", fontsize=18)\nax.tick_params(axis=\"both\", labelsize=12)\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.747403Z","iopub.execute_input":"2025-12-04T12:51:47.747653Z","iopub.status.idle":"2025-12-04T12:51:47.998743Z","shell.execute_reply.started":"2025-12-04T12:51:47.747632Z","shell.execute_reply":"2025-12-04T12:51:47.998061Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"importance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': models['XGB'].feature_importances_\n}).sort_values(by='importance', ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:47.999551Z","iopub.execute_input":"2025-12-04T12:51:48.000019Z","iopub.status.idle":"2025-12-04T12:51:48.005960Z","shell.execute_reply.started":"2025-12-04T12:51:47.999993Z","shell.execute_reply":"2025-12-04T12:51:48.005155Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"importance_df ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:48.006983Z","iopub.execute_input":"2025-12-04T12:51:48.007291Z","iopub.status.idle":"2025-12-04T12:51:48.025674Z","shell.execute_reply.started":"2025-12-04T12:51:48.007263Z","shell.execute_reply":"2025-12-04T12:51:48.024751Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                            feature  importance\n18                    pos_diversity    0.449504\n20        sentence_length_variation    0.094889\n0                   character_count    0.032498\n27                pos_2gram_variety    0.030969\n19                      hapax_ratio    0.027857\n12              flesch_reading_ease    0.027059\n7               sentence_complexity    0.025031\n28                pos_3gram_variety    0.020899\n9        sentence_length_difference    0.019528\n8                 punctuation_count    0.019067\n5            sentiment_subjectivity    0.018478\n26             grammatical_mistakes    0.017863\n22                specificity_score    0.016178\n23        figurative_language_score    0.016105\n2                   paragraph_count    0.015931\n10                 type_token_ratio    0.015076\n3                    stopword_count    0.014413\n29                pos_4gram_variety    0.014210\n21                  repetition_rate    0.013443\n17                    pronoun_ratio    0.013381\n13                       gzip_ratio    0.013197\n11                     word_entropy    0.012536\n1                        word_count    0.012352\n25         transition_variety_score    0.010267\n14              question_stmt_ratio    0.009542\n24  paragraph_coherence_consistency    0.009065\n6            discourse_marker_count    0.008438\n15            clause_sentence_ratio    0.007846\n16                       modal_freq    0.007241\n4                 unique_word_count    0.007139","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>pos_diversity</td>\n      <td>0.449504</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>sentence_length_variation</td>\n      <td>0.094889</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>character_count</td>\n      <td>0.032498</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>pos_2gram_variety</td>\n      <td>0.030969</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>hapax_ratio</td>\n      <td>0.027857</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>flesch_reading_ease</td>\n      <td>0.027059</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sentence_complexity</td>\n      <td>0.025031</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>pos_3gram_variety</td>\n      <td>0.020899</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sentence_length_difference</td>\n      <td>0.019528</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>punctuation_count</td>\n      <td>0.019067</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sentiment_subjectivity</td>\n      <td>0.018478</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>grammatical_mistakes</td>\n      <td>0.017863</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>specificity_score</td>\n      <td>0.016178</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>figurative_language_score</td>\n      <td>0.016105</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>paragraph_count</td>\n      <td>0.015931</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>type_token_ratio</td>\n      <td>0.015076</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>stopword_count</td>\n      <td>0.014413</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>pos_4gram_variety</td>\n      <td>0.014210</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>repetition_rate</td>\n      <td>0.013443</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>pronoun_ratio</td>\n      <td>0.013381</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>gzip_ratio</td>\n      <td>0.013197</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>word_entropy</td>\n      <td>0.012536</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>word_count</td>\n      <td>0.012352</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>transition_variety_score</td>\n      <td>0.010267</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>question_stmt_ratio</td>\n      <td>0.009542</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>paragraph_coherence_consistency</td>\n      <td>0.009065</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>discourse_marker_count</td>\n      <td>0.008438</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>clause_sentence_ratio</td>\n      <td>0.007846</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>modal_freq</td>\n      <td>0.007241</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>unique_word_count</td>\n      <td>0.007139</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# %%capture\n# !pip install pipreqs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:48.026491Z","iopub.execute_input":"2025-12-04T12:51:48.026740Z","iopub.status.idle":"2025-12-04T12:51:48.040630Z","shell.execute_reply.started":"2025-12-04T12:51:48.026720Z","shell.execute_reply":"2025-12-04T12:51:48.039543Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# !pipreqs .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:48.041706Z","iopub.execute_input":"2025-12-04T12:51:48.042113Z","iopub.status.idle":"2025-12-04T12:51:48.055748Z","shell.execute_reply.started":"2025-12-04T12:51:48.042081Z","shell.execute_reply":"2025-12-04T12:51:48.054644Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(models['XGB'])\nshap_values = explainer.shap_values(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:51:48.056825Z","iopub.execute_input":"2025-12-04T12:51:48.057126Z","iopub.status.idle":"2025-12-04T12:52:04.576605Z","shell.execute_reply.started":"2025-12-04T12:51:48.057100Z","shell.execute_reply":"2025-12-04T12:52:04.575926Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"* **Red** = higher feature values\n\n* **Blue** = lower feature values\n\n* **Right** = pushes toward AI-generated label (1)\n\n* **Left** = pushes toward human-written label (0)","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_train)\nplt.gcf().savefig('shap_summary_plot.png', dpi=300, bbox_inches='tight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:04.577464Z","iopub.execute_input":"2025-12-04T12:52:04.578488Z","iopub.status.idle":"2025-12-04T12:52:11.734219Z","shell.execute_reply.started":"2025-12-04T12:52:04.578453Z","shell.execute_reply":"2025-12-04T12:52:11.733545Z"}},"outputs":[{"name":"stderr","text":"The figure layout has changed to tight\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nshap.summary_plot(shap_values, X_train, plot_type='bar', show=False)\nplt.tight_layout()\nplt.savefig('shap_global_importance.png', dpi=300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:11.735145Z","iopub.execute_input":"2025-12-04T12:52:11.735453Z","iopub.status.idle":"2025-12-04T12:52:12.340267Z","shell.execute_reply.started":"2025-12-04T12:52:11.735427Z","shell.execute_reply":"2025-12-04T12:52:12.339351Z"}},"outputs":[{"name":"stderr","text":"The figure layout has changed to tight\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Sentencewise probability of being AI-generated","metadata":{}},{"cell_type":"code","source":"!pip install textstat -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:12.341180Z","iopub.execute_input":"2025-12-04T12:52:12.341456Z","iopub.status.idle":"2025-12-04T12:52:16.981734Z","shell.execute_reply.started":"2025-12-04T12:52:12.341436Z","shell.execute_reply":"2025-12-04T12:52:16.980677Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import polars as pl\nimport re\nimport string\nfrom collections import Counter\nfrom textblob import TextBlob\nimport textstat\nimport spacy\nfrom scipy.stats import entropy\nimport gzip\nfrom nltk.corpus import stopwords\nimport shap\nimport xgboost as xgb\nfrom nltk.tokenize import sent_tokenize\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nimport re\n\nnlp = spacy.load(\"en_core_web_sm\")\nstop_words = set(stopwords.words('english'))\n\n# Define all feature functions\ndef vocabSize(sentence):\n    doc = nlp(sentence.lower())\n    tokens = set([token.text for token in doc if not token.is_punct])\n    return len(tokens)\n\ndef sentence_complexity(sentence):\n    flesch_score = textstat.flesch_reading_ease(sentence)\n    fk_grade_level = textstat.flesch_kincaid_grade(sentence)\n    gunning_fog = textstat.gunning_fog(sentence)\n    smog_index = textstat.smog_index(sentence)\n    composite_score = (flesch_score * 0.2 + fk_grade_level * 0.3 + \n                      gunning_fog * 0.3 + smog_index * 0.2)\n    return composite_score\n\ndef punctuation_count(paragraph):\n    return sum(1 for char in paragraph if char in string.punctuation)\n\ndef sentence_length_difference(paragraph):\n    sentences = re.split(r'[.!?]', paragraph)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    if not sentences:\n        return 0\n    sentence_lengths = [len(s.split()) for s in sentences]\n    return max(sentence_lengths) - min(sentence_lengths)\n\ndef type_token_ratio(text):\n    words = text.split()\n    if len(words) == 0:\n        return 0\n    unique_words = set(words)\n    return len(unique_words) / len(words)\n\ndef pos_counts(text):\n    doc = nlp(text)\n    pos_count_dict = {}\n    for token in doc:\n        pos = token.pos_\n        pos_count_dict[pos] = pos_count_dict.get(pos, 0) + 1\n    return pos_count_dict\n\ndef count_discourse_markers(text):\n    discourse_markers = [\"however\", \"therefore\", \"although\", \"nevertheless\", \"hence\"]\n    return sum(text.lower().count(marker) for marker in discourse_markers)\n\ndef word_entropy(text):\n    doc = nlp(text)\n    words = [t.lemma_.lower() for t in doc if t.is_alpha]\n    if not words:\n        return 0\n    freqs = list(Counter(words).values())\n    return entropy(freqs)\n\ndef flesch_reading_ease(text):\n    try:\n        return textstat.flesch_reading_ease(text)\n    except:\n        return 0\n\ndef gzip_ratio(text):\n    if len(text) == 0:\n        return 0\n    compressed = len(gzip.compress(text.encode('utf-8')))\n    return compressed / len(text)\n\ndef negation_frequency(text):\n    doc = nlp(text)\n    negations = [t for t in doc if t.dep_ == \"neg\" or \n                 t.lemma_.lower() in [\"not\", \"no\", \"never\", \"none\", \"n't\"]]\n    total_words = len([t for t in doc if t.is_alpha])\n    return len(negations) / (total_words + 1e-5)\n\ndef question_statement_ratio(text):\n    doc = nlp(text)\n    sentences = list(doc.sents)\n    if not sentences:\n        return 0\n    question_count = sum(1 for s in sentences if s.text.strip().endswith(\"?\"))\n    statement_count = sum(1 for s in sentences if s.text.strip().endswith(\".\"))\n    return question_count / (statement_count + 1e-5)\n\ndef clause_to_sentence_ratio(text):\n    doc = nlp(text)\n    sentences = list(doc.sents)\n    if not sentences:\n        return 0\n    clause_markers = (\"mark\", \"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\", \"conj\")\n    clause_count = sum(1 for t in doc if t.dep_ in clause_markers)\n    return clause_count / len(sentences)\n\ndef modal_verb_frequency(text):\n    doc = nlp(text)\n    modals = {\"can\", \"could\", \"may\", \"might\", \"shall\", \"should\", \"will\", \"would\", \"must\"}\n    modal_count = sum(1 for t in doc if t.lemma_.lower() in modals)\n    total_words = len([t for t in doc if t.is_alpha])\n    return modal_count / (total_words + 1e-5)\n\ndef pronoun_ratio(text):\n    doc = nlp(text)\n    pronouns = [t for t in doc if t.pos_ == \"PRON\"]\n    total_words = len([t for t in doc if t.is_alpha])\n    return len(pronouns) / (total_words + 1e-5)\n\ndef pos_diversity(text):\n    doc = nlp(text)\n    pos_tags = [t.pos_ for t in doc if t.is_alpha]\n    if not pos_tags:\n        return 0\n    counts = Counter(pos_tags)\n    return entropy(list(counts.values()))\n\ndef hapax_legomena_ratio(text):\n    doc = nlp(text)\n    words = [t.lemma_.lower() for t in doc if t.is_alpha]\n    if not words:\n        return 0\n    freqs = Counter(words)\n    hapax = sum(1 for w, f in freqs.items() if f == 1)\n    return hapax / len(freqs)\n\ndef get_sentiment_polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndef get_sentiment_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef count_stopwords(text):\n    return len([word for word in text.split() if word.lower() in stop_words])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:16.983189Z","iopub.execute_input":"2025-12-04T12:52:16.983500Z","iopub.status.idle":"2025-12-04T12:52:18.216277Z","shell.execute_reply.started":"2025-12-04T12:52:16.983473Z","shell.execute_reply":"2025-12-04T12:52:18.215428Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"ai_text = \"Quantum mechanics is the branch of physics that describes the behaavior of matter and energy at the smallest scales, such as atoms, electrons, and photons. Unlike classical physics, which assumes particles have definite positions and velocities, quantum mechanics reveals that particles can exist in superpositions—being in multiple states at once—and their behavior is fundamentally probabilistic. Key concepts include wave-particle duality, where particles behave like both waves and particles, and entanglement, where particles become linked so that the state of one instantly affects the other, even across great distances. The theory is mathematically described using wavefunctions and has led to groundbreaking technologies like lasers, semiconductors, and quantum computing.\"\nai_text_label = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:18.217203Z","iopub.execute_input":"2025-12-04T12:52:18.217457Z","iopub.status.idle":"2025-12-04T12:52:18.221457Z","shell.execute_reply.started":"2025-12-04T12:52:18.217438Z","shell.execute_reply":"2025-12-04T12:52:18.220735Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# human_text = train_df[['text','label']].iloc[23704]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:18.222254Z","iopub.execute_input":"2025-12-04T12:52:18.222521Z","iopub.status.idle":"2025-12-04T12:52:18.239870Z","shell.execute_reply.started":"2025-12-04T12:52:18.222504Z","shell.execute_reply":"2025-12-04T12:52:18.239090Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Evaluating on COLING","metadata":{}},{"cell_type":"code","source":"coling_test = pd.read_csv('/kaggle/input/creating-with-new-features-coling/coling_test3000.csv')\ncoling_test_data = coling_test[FEATURES]\ncoling_test_label = coling_test[TARGET]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:18.240666Z","iopub.execute_input":"2025-12-04T12:52:18.240880Z","iopub.status.idle":"2025-12-04T12:52:18.918770Z","shell.execute_reply.started":"2025-12-04T12:52:18.240863Z","shell.execute_reply":"2025-12-04T12:52:18.917930Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"coling_test_data.to_csv('coling.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:18.923637Z","iopub.execute_input":"2025-12-04T12:52:18.923894Z","iopub.status.idle":"2025-12-04T12:52:19.284243Z","shell.execute_reply.started":"2025-12-04T12:52:18.923873Z","shell.execute_reply":"2025-12-04T12:52:19.283628Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"coling_results = []\ncoling_pred = {}\nfor model_name, model in models.items():\n    print(f\"Predicting with {model_name}...\")\n    \n    # Train the model\n    # model.fit(X_train, y_train)\n\n    # Predict on test data\n    y_pred_coling = model.predict(coling_test_data)\n    coling_pred[model_name] = y_pred_coling\n\n    # Calculate metrics\n    accuracy = accuracy_score(coling_test_label, y_pred_coling)\n    precision = precision_score(coling_test_label, y_pred_coling)\n    recall = recall_score(coling_test_label, y_pred_coling)\n    f1 = f1_score(coling_test_label, y_pred_coling)\n\n    # Save the model using joblib\n    # joblib.dump(model, f\"{model_name.replace(' ', '_')}_model.joblib\")\n\n    coling_results.append({\n        'Model': model_name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1 Score': f1\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:19.285136Z","iopub.execute_input":"2025-12-04T12:52:19.285359Z","iopub.status.idle":"2025-12-04T12:52:21.728786Z","shell.execute_reply.started":"2025-12-04T12:52:19.285340Z","shell.execute_reply":"2025-12-04T12:52:21.727946Z"}},"outputs":[{"name":"stdout","text":"Predicting with XGB...\nPredicting with Random Forest...\nPredicting with Logistic Regression...\nPredicting with SVM...\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"coling_result_df = pd.DataFrame(coling_results)\ncoling_result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:21.729630Z","iopub.execute_input":"2025-12-04T12:52:21.729868Z","iopub.status.idle":"2025-12-04T12:52:21.740522Z","shell.execute_reply.started":"2025-12-04T12:52:21.729842Z","shell.execute_reply":"2025-12-04T12:52:21.739798Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                 Model  Accuracy  Precision    Recall  F1 Score\n0                  XGB    0.6073   0.701283  0.652707  0.676124\n1        Random Forest    0.6449   0.673226  0.844427  0.749170\n2  Logistic Regression    0.6870   0.741713  0.769586  0.755392\n3                  SVM    0.6452   0.713772  0.726274  0.719968","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGB</td>\n      <td>0.6073</td>\n      <td>0.701283</td>\n      <td>0.652707</td>\n      <td>0.676124</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random Forest</td>\n      <td>0.6449</td>\n      <td>0.673226</td>\n      <td>0.844427</td>\n      <td>0.749170</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Logistic Regression</td>\n      <td>0.6870</td>\n      <td>0.741713</td>\n      <td>0.769586</td>\n      <td>0.755392</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SVM</td>\n      <td>0.6452</td>\n      <td>0.713772</td>\n      <td>0.726274</td>\n      <td>0.719968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# # Convert to numpy array for easier manipulation\n# preds_array_coling = np.array(coling_preds)\n\n# # Get majority prediction - axis=0 calculates mode across models for each sample\n# majority_result_coling = mode(preds_array_coling, axis=0)\n\n# # Extract just the majority predictions (ignore the counts)\n# majority_predictions_coling = majority_result_coling.mode\n# print(f\"The f1 score after majority voting is {f1_score(coling_test_label, majority_predictions_coling)}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:21.741359Z","iopub.execute_input":"2025-12-04T12:52:21.741661Z","iopub.status.idle":"2025-12-04T12:52:21.757212Z","shell.execute_reply.started":"2025-12-04T12:52:21.741641Z","shell.execute_reply":"2025-12-04T12:52:21.756386Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Analysing the error in prediction for COLING test","metadata":{}},{"cell_type":"code","source":"import shap\nimport numpy as np\nimport pandas as pd\n\ndef shap_error_diagnostics(shap_values, preds, labels, feature_names, threshold=0.5):\n    \"\"\"\n    SHAP-based diagnostic analysis:\n    Splits predictions into TP, TN, FP, FN and computes mean absolute SHAP values.\n\n    Parameters\n    ----------\n    shap_values : np.ndarray (n_samples, n_features)\n        SHAP values from TreeExplainer or model-specific explainer.\n    preds : np.ndarray (n_samples,)\n        Model output probabilities.\n    labels : np.ndarray (n_samples,)\n        Ground truth binary labels (0 = human, 1 = AI).\n    feature_names : list\n        Names of features in correct order.\n    threshold : float\n        Decision threshold for converting prob → class.\n\n    Returns\n    -------\n    pd.DataFrame:\n        Diagnostic table with mean |SHAP| for TP, TN, FP, FN.\n\n    Also prints:\n        - Counts of each prediction type\n        - Short interpretation notes\n    \"\"\"\n\n    # Convert probabilities to hard predictions\n    pred_labels = (preds >= threshold).astype(int)\n\n    # Define groups\n    TP_idx = (labels == 1) & (pred_labels == 1)\n    TN_idx = (labels == 0) & (pred_labels == 0)\n    FP_idx = (labels == 0) & (pred_labels == 1)\n    FN_idx = (labels == 1) & (pred_labels == 0)\n\n    # Helper function\n    def mean_abs_shap(mask):\n        if mask.sum() == 0:\n            return np.zeros(shap_values.shape[1])\n        return np.abs(shap_values[mask]).mean(axis=0)\n\n    # Compute statistics\n    shap_TP = mean_abs_shap(TP_idx)\n    shap_TN = mean_abs_shap(TN_idx)\n    shap_FP = mean_abs_shap(FP_idx)\n    shap_FN = mean_abs_shap(FN_idx)\n\n    # Build DataFrame\n    df = pd.DataFrame({\n        \"TP_mean_abs_SHAP\": shap_TP,\n        \"TN_mean_abs_SHAP\": shap_TN,\n        \"FP_mean_abs_SHAP\": shap_FP,\n        \"FN_mean_abs_SHAP\": shap_FN,\n    }, index=feature_names)\n\n    # Sort features by mistake importance: FP + FN\n    df[\"Error_Impact\"] = df[\"FP_mean_abs_SHAP\"] + df[\"FN_mean_abs_SHAP\"]\n    df = df.sort_values(\"Error_Impact\", ascending=False)\n\n    # Print useful diagnostics\n    print(\"\\n===== Prediction Breakdown =====\")\n    print(f\"TP: {TP_idx.sum()}  (AI → AI)\")\n    print(f\"TN: {TN_idx.sum()}  (Human → Human)\")\n    print(f\"FP: {FP_idx.sum()}  (Human → AI)\")\n    print(f\"FN: {FN_idx.sum()}  (AI → Human)\")\n\n    print(\"\\n===== Interpretation =====\")\n    print(\"• Features with high FP_mean_abs_SHAP tend to *falsely push Human text toward AI*.\")\n    print(\"• Features with high FN_mean_abs_SHAP tend to *falsely push AI text toward Human*.\")\n    print(\"• Error_Impact = contribution to total model confusion.\")\n    # print(\"• Inspect top rows of the returned DataFrame for the root causes of errors.\\n\")\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:21.758118Z","iopub.execute_input":"2025-12-04T12:52:21.758415Z","iopub.status.idle":"2025-12-04T12:52:21.775666Z","shell.execute_reply.started":"2025-12-04T12:52:21.758363Z","shell.execute_reply":"2025-12-04T12:52:21.774845Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Get predictions\ncoling_xgb_preds = models['XGB'].predict(coling_test_data)\n\n# Compute confusion matrix\ncm = confusion_matrix(coling_test_label, coling_xgb_preds, labels=[0,1])\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Human (0)', 'AI (1)'])\nfig, ax = plt.subplots(figsize=(3, 3))\ndisp.plot(cmap='Blues', ax=ax)\nplt.title(\"Confusion Matrix: XGB on COLING Dataset\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:21.776837Z","iopub.execute_input":"2025-12-04T12:52:21.777135Z","iopub.status.idle":"2025-12-04T12:52:21.899929Z","shell.execute_reply.started":"2025-12-04T12:52:21.777108Z","shell.execute_reply":"2025-12-04T12:52:21.899178Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"coling_shap_values = explainer.shap_values(coling_test_data)\ndiagnostics = shap_error_diagnostics(\n    shap_values=coling_shap_values, \n    preds=coling_xgb_preds, \n    labels=coling_test_label, \n    feature_names=FEATURES\n)\n\ndiagnostics.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:21.900788Z","iopub.execute_input":"2025-12-04T12:52:21.901063Z","iopub.status.idle":"2025-12-04T12:52:28.076212Z","shell.execute_reply.started":"2025-12-04T12:52:21.901038Z","shell.execute_reply":"2025-12-04T12:52:28.075509Z"}},"outputs":[{"name":"stdout","text":"\n===== Prediction Breakdown =====\nTP: 4099  (AI → AI)\nTN: 1974  (Human → Human)\nFP: 1746  (Human → AI)\nFN: 2181  (AI → Human)\n\n===== Interpretation =====\n• Features with high FP_mean_abs_SHAP tend to *falsely push Human text toward AI*.\n• Features with high FN_mean_abs_SHAP tend to *falsely push AI text toward Human*.\n• Error_Impact = contribution to total model confusion.\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                 TP_mean_abs_SHAP  TN_mean_abs_SHAP  \\\npunctuation_count                        1.952939          1.932581   \nhapax_ratio                              1.474939          1.339203   \nsentence_length_variation                1.575001          1.285465   \npos_diversity                            1.389129          1.326709   \nflesch_reading_ease                      1.088570          0.953699   \nparagraph_coherence_consistency          0.836466          0.865804   \nparagraph_count                          0.901577          0.943831   \nstopword_count                           0.951979          0.900190   \npos_4gram_variety                        0.861034          0.879955   \npos_3gram_variety                        0.728782          0.726945   \ngrammatical_mistakes                     0.934762          0.569001   \npronoun_ratio                            0.662095          0.732018   \nword_entropy                             0.627034          0.590986   \ngzip_ratio                               0.544106          0.562929   \nrepetition_rate                          0.586510          0.495374   \n\n                                 FP_mean_abs_SHAP  FN_mean_abs_SHAP  \\\npunctuation_count                        1.938293          2.261217   \nhapax_ratio                              1.508095          1.581114   \nsentence_length_variation                1.384193          1.363563   \npos_diversity                            1.457037          1.282611   \nflesch_reading_ease                      1.126377          0.940731   \nparagraph_coherence_consistency          0.958302          1.041406   \nparagraph_count                          0.725337          1.268267   \nstopword_count                           0.985878          0.872870   \npos_4gram_variety                        0.820214          0.829802   \npos_3gram_variety                        0.708541          0.725128   \ngrammatical_mistakes                     0.719265          0.674727   \npronoun_ratio                            0.656600          0.716932   \nword_entropy                             0.646724          0.611748   \ngzip_ratio                               0.648146          0.552615   \nrepetition_rate                          0.456511          0.718940   \n\n                                 Error_Impact  \npunctuation_count                    4.199510  \nhapax_ratio                          3.089209  \nsentence_length_variation            2.747756  \npos_diversity                        2.739648  \nflesch_reading_ease                  2.067109  \nparagraph_coherence_consistency      1.999708  \nparagraph_count                      1.993604  \nstopword_count                       1.858748  \npos_4gram_variety                    1.650016  \npos_3gram_variety                    1.433668  \ngrammatical_mistakes                 1.393992  \npronoun_ratio                        1.373532  \nword_entropy                         1.258472  \ngzip_ratio                           1.200761  \nrepetition_rate                      1.175451  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TP_mean_abs_SHAP</th>\n      <th>TN_mean_abs_SHAP</th>\n      <th>FP_mean_abs_SHAP</th>\n      <th>FN_mean_abs_SHAP</th>\n      <th>Error_Impact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>punctuation_count</th>\n      <td>1.952939</td>\n      <td>1.932581</td>\n      <td>1.938293</td>\n      <td>2.261217</td>\n      <td>4.199510</td>\n    </tr>\n    <tr>\n      <th>hapax_ratio</th>\n      <td>1.474939</td>\n      <td>1.339203</td>\n      <td>1.508095</td>\n      <td>1.581114</td>\n      <td>3.089209</td>\n    </tr>\n    <tr>\n      <th>sentence_length_variation</th>\n      <td>1.575001</td>\n      <td>1.285465</td>\n      <td>1.384193</td>\n      <td>1.363563</td>\n      <td>2.747756</td>\n    </tr>\n    <tr>\n      <th>pos_diversity</th>\n      <td>1.389129</td>\n      <td>1.326709</td>\n      <td>1.457037</td>\n      <td>1.282611</td>\n      <td>2.739648</td>\n    </tr>\n    <tr>\n      <th>flesch_reading_ease</th>\n      <td>1.088570</td>\n      <td>0.953699</td>\n      <td>1.126377</td>\n      <td>0.940731</td>\n      <td>2.067109</td>\n    </tr>\n    <tr>\n      <th>paragraph_coherence_consistency</th>\n      <td>0.836466</td>\n      <td>0.865804</td>\n      <td>0.958302</td>\n      <td>1.041406</td>\n      <td>1.999708</td>\n    </tr>\n    <tr>\n      <th>paragraph_count</th>\n      <td>0.901577</td>\n      <td>0.943831</td>\n      <td>0.725337</td>\n      <td>1.268267</td>\n      <td>1.993604</td>\n    </tr>\n    <tr>\n      <th>stopword_count</th>\n      <td>0.951979</td>\n      <td>0.900190</td>\n      <td>0.985878</td>\n      <td>0.872870</td>\n      <td>1.858748</td>\n    </tr>\n    <tr>\n      <th>pos_4gram_variety</th>\n      <td>0.861034</td>\n      <td>0.879955</td>\n      <td>0.820214</td>\n      <td>0.829802</td>\n      <td>1.650016</td>\n    </tr>\n    <tr>\n      <th>pos_3gram_variety</th>\n      <td>0.728782</td>\n      <td>0.726945</td>\n      <td>0.708541</td>\n      <td>0.725128</td>\n      <td>1.433668</td>\n    </tr>\n    <tr>\n      <th>grammatical_mistakes</th>\n      <td>0.934762</td>\n      <td>0.569001</td>\n      <td>0.719265</td>\n      <td>0.674727</td>\n      <td>1.393992</td>\n    </tr>\n    <tr>\n      <th>pronoun_ratio</th>\n      <td>0.662095</td>\n      <td>0.732018</td>\n      <td>0.656600</td>\n      <td>0.716932</td>\n      <td>1.373532</td>\n    </tr>\n    <tr>\n      <th>word_entropy</th>\n      <td>0.627034</td>\n      <td>0.590986</td>\n      <td>0.646724</td>\n      <td>0.611748</td>\n      <td>1.258472</td>\n    </tr>\n    <tr>\n      <th>gzip_ratio</th>\n      <td>0.544106</td>\n      <td>0.562929</td>\n      <td>0.648146</td>\n      <td>0.552615</td>\n      <td>1.200761</td>\n    </tr>\n    <tr>\n      <th>repetition_rate</th>\n      <td>0.586510</td>\n      <td>0.495374</td>\n      <td>0.456511</td>\n      <td>0.718940</td>\n      <td>1.175451</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pan_shap_values = explainer.shap_values(X_test)\ndiagnostics = shap_error_diagnostics(\n    shap_values=pan_shap_values, \n    preds=pan_xgb_preds, \n    labels=y_test, \n    feature_names=FEATURES\n)\n\ndiagnostics.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:28.077146Z","iopub.execute_input":"2025-12-04T12:52:28.077476Z","iopub.status.idle":"2025-12-04T12:52:30.299532Z","shell.execute_reply.started":"2025-12-04T12:52:28.077448Z","shell.execute_reply":"2025-12-04T12:52:30.298725Z"}},"outputs":[{"name":"stdout","text":"\n===== Prediction Breakdown =====\nTP: 2264  (AI → AI)\nTN: 1208  (Human → Human)\nFP: 69  (Human → AI)\nFN: 48  (AI → Human)\n\n===== Interpretation =====\n• Features with high FP_mean_abs_SHAP tend to *falsely push Human text toward AI*.\n• Features with high FN_mean_abs_SHAP tend to *falsely push AI text toward Human*.\n• Error_Impact = contribution to total model confusion.\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                           TP_mean_abs_SHAP  TN_mean_abs_SHAP  \\\nsentence_length_variation          1.495031          1.497398   \npos_diversity                      1.394320          1.802645   \npunctuation_count                  0.855919          0.824223   \nhapax_ratio                        0.973178          0.723776   \nstopword_count                     0.874833          0.841020   \npronoun_ratio                      0.765402          0.805388   \npos_4gram_variety                  0.735730          0.789726   \nflesch_reading_ease                0.928309          0.730473   \nparagraph_count                    0.635617          0.499751   \ngrammatical_mistakes               0.607259          0.497433   \ncharacter_count                    0.568958          0.898897   \nrepetition_rate                    0.555572          0.516581   \nsentiment_subjectivity             0.396457          0.414123   \npos_3gram_variety                  0.487771          0.447109   \ngzip_ratio                         0.384687          0.447715   \n\n                           FP_mean_abs_SHAP  FN_mean_abs_SHAP  Error_Impact  \nsentence_length_variation          1.284666          1.635083      2.919749  \npos_diversity                      1.223740          1.316914      2.540654  \npunctuation_count                  1.055757          0.951045      2.006802  \nhapax_ratio                        1.031534          0.925142      1.956676  \nstopword_count                     0.915371          0.736026      1.651397  \npronoun_ratio                      0.768583          0.861827      1.630410  \npos_4gram_variety                  0.793160          0.743026      1.536186  \nflesch_reading_ease                0.717897          0.785299      1.503196  \nparagraph_count                    0.674721          0.540561      1.215282  \ngrammatical_mistakes               0.539248          0.536284      1.075533  \ncharacter_count                    0.425733          0.649211      1.074943  \nrepetition_rate                    0.557259          0.479791      1.037050  \nsentiment_subjectivity             0.475839          0.492670      0.968510  \npos_3gram_variety                  0.503374          0.429113      0.932487  \ngzip_ratio                         0.416009          0.392067      0.808076  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TP_mean_abs_SHAP</th>\n      <th>TN_mean_abs_SHAP</th>\n      <th>FP_mean_abs_SHAP</th>\n      <th>FN_mean_abs_SHAP</th>\n      <th>Error_Impact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sentence_length_variation</th>\n      <td>1.495031</td>\n      <td>1.497398</td>\n      <td>1.284666</td>\n      <td>1.635083</td>\n      <td>2.919749</td>\n    </tr>\n    <tr>\n      <th>pos_diversity</th>\n      <td>1.394320</td>\n      <td>1.802645</td>\n      <td>1.223740</td>\n      <td>1.316914</td>\n      <td>2.540654</td>\n    </tr>\n    <tr>\n      <th>punctuation_count</th>\n      <td>0.855919</td>\n      <td>0.824223</td>\n      <td>1.055757</td>\n      <td>0.951045</td>\n      <td>2.006802</td>\n    </tr>\n    <tr>\n      <th>hapax_ratio</th>\n      <td>0.973178</td>\n      <td>0.723776</td>\n      <td>1.031534</td>\n      <td>0.925142</td>\n      <td>1.956676</td>\n    </tr>\n    <tr>\n      <th>stopword_count</th>\n      <td>0.874833</td>\n      <td>0.841020</td>\n      <td>0.915371</td>\n      <td>0.736026</td>\n      <td>1.651397</td>\n    </tr>\n    <tr>\n      <th>pronoun_ratio</th>\n      <td>0.765402</td>\n      <td>0.805388</td>\n      <td>0.768583</td>\n      <td>0.861827</td>\n      <td>1.630410</td>\n    </tr>\n    <tr>\n      <th>pos_4gram_variety</th>\n      <td>0.735730</td>\n      <td>0.789726</td>\n      <td>0.793160</td>\n      <td>0.743026</td>\n      <td>1.536186</td>\n    </tr>\n    <tr>\n      <th>flesch_reading_ease</th>\n      <td>0.928309</td>\n      <td>0.730473</td>\n      <td>0.717897</td>\n      <td>0.785299</td>\n      <td>1.503196</td>\n    </tr>\n    <tr>\n      <th>paragraph_count</th>\n      <td>0.635617</td>\n      <td>0.499751</td>\n      <td>0.674721</td>\n      <td>0.540561</td>\n      <td>1.215282</td>\n    </tr>\n    <tr>\n      <th>grammatical_mistakes</th>\n      <td>0.607259</td>\n      <td>0.497433</td>\n      <td>0.539248</td>\n      <td>0.536284</td>\n      <td>1.075533</td>\n    </tr>\n    <tr>\n      <th>character_count</th>\n      <td>0.568958</td>\n      <td>0.898897</td>\n      <td>0.425733</td>\n      <td>0.649211</td>\n      <td>1.074943</td>\n    </tr>\n    <tr>\n      <th>repetition_rate</th>\n      <td>0.555572</td>\n      <td>0.516581</td>\n      <td>0.557259</td>\n      <td>0.479791</td>\n      <td>1.037050</td>\n    </tr>\n    <tr>\n      <th>sentiment_subjectivity</th>\n      <td>0.396457</td>\n      <td>0.414123</td>\n      <td>0.475839</td>\n      <td>0.492670</td>\n      <td>0.968510</td>\n    </tr>\n    <tr>\n      <th>pos_3gram_variety</th>\n      <td>0.487771</td>\n      <td>0.447109</td>\n      <td>0.503374</td>\n      <td>0.429113</td>\n      <td>0.932487</td>\n    </tr>\n    <tr>\n      <th>gzip_ratio</th>\n      <td>0.384687</td>\n      <td>0.447715</td>\n      <td>0.416009</td>\n      <td>0.392067</td>\n      <td>0.808076</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"## SHAP plot for COLING test","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(coling_shap_values, coling_test_data)\nplt.savefig('shap_summary_plot_coling.png', dpi=300, bbox_inches='tight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:30.300483Z","iopub.execute_input":"2025-12-04T12:52:30.300772Z","iopub.status.idle":"2025-12-04T12:52:34.352866Z","shell.execute_reply.started":"2025-12-04T12:52:30.300740Z","shell.execute_reply":"2025-12-04T12:52:34.352210Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"## Installing language-tool-python compatible for kaggle\n!pip install -q /kaggle/input/language-tool-python-2-7-1/language_tool_python-2.7.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:34.354149Z","iopub.execute_input":"2025-12-04T12:52:34.354497Z","iopub.status.idle":"2025-12-04T12:52:37.988026Z","shell.execute_reply.started":"2025-12-04T12:52:34.354466Z","shell.execute_reply":"2025-12-04T12:52:37.986779Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# %% Directory settings\n\n# ====================================================\n# Directory settings\n# ====================================================\nfrom pathlib import Path\nimport re\n\nINPUT_DIR = Path(\"../input/\")\n\nimport os\nimport zipfile\nfrom zipfile import ZipFile\nimport shutil\n\n# create download path\ndef get_language_tool_cache_path():\n    # Get download path from environment or use default.\n    download_path = os.environ.get(\n        'LTP_PATH',\n        os.path.join(os.path.expanduser(\"~\"), \".cache\", \"language_tool_python\")\n    )\n    # Make download path, if it doesn't exist.\n    os.makedirs(download_path, exist_ok=True)\n    return download_path\n\nlt_path = get_language_tool_cache_path()\n# lt_path\n\ndef get_all_file_paths(directory):\n  \n    # initializing empty file paths list\n    file_paths = []\n  \n    # crawling through directory and subdirectories\n    for root, directories, files in os.walk(directory):\n        for filename in files:\n            # join the two strings in order to form the full filepath.\n            filepath = os.path.join(root, filename)\n            file_paths.append(filepath)\n  \n    # returning all file paths\n    return file_paths        \n  \ndef main():\n    # path to folder which needs to be zipped\n    directory = '../input/language-tool-python-2-7-1/LanguageTool-5.7/LanguageTool-5.7'\n  \n    # calling function to get all file paths in the directory\n    file_paths = get_all_file_paths(directory)\n\n    # writing files to a zipfile\n    with ZipFile('./lt.zip','w') as zip:\n        # writing each file one by one\n        for file in file_paths:\n            zip.write(file)\n  \n    print('All files zipped successfully!')        \n    \nmain()\n\n\n \nzip_file = \"./lt.zip\"\n \ntry:\n    with zipfile.ZipFile(zip_file) as z:\n        z.extractall()\n        print(\"Extracted all\")\nexcept:\n    print(\"Invalid file\")\n    \n#move to cache\n!mv {'./input/language-tool-python-2-7-1/LanguageTool-5.7/LanguageTool-5.7'} {lt_path} \nprint(os.listdir('/root/.cache/language_tool_python/'))\n\n#remove files from output\n\nshutil.rmtree('./input')\nos.remove(\"./lt.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:52:37.989527Z","iopub.execute_input":"2025-12-04T12:52:37.989897Z","iopub.status.idle":"2025-12-04T12:53:01.682425Z","shell.execute_reply.started":"2025-12-04T12:52:37.989861Z","shell.execute_reply":"2025-12-04T12:53:01.681351Z"}},"outputs":[{"name":"stdout","text":"All files zipped successfully!\nExtracted all\n['LanguageTool-5.7']\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import language_tool_python\n\ntool = language_tool_python.LanguageTool('en-US')\n\ndef grammatical_mistakes(sentence):\n\n  mistakes = len(tool.check(sentence))\n\n  return mistakes\n\ndef pos_tag_ngrams(text, n=2):\n    doc = nlp(text)\n    tags = [token.pos_ for token in doc if token.is_alpha]\n\n    if len(tags) < n:\n        return {}\n\n    ngrams = zip(*[tags[i:] for i in range(n)])\n    return Counter(ngrams)\n\ndef pos_ngram_variety(text, n=2):\n    ngrams = pos_tag_ngrams(text, n)\n    return len(ngrams)\n\n\n# ========== Feature Extraction ==========\ndef get_features_from_text(text: str):\n    stop_words = set(stopwords.words('english'))\n\n    data = {\n        'character_count': len(text),\n        'word_count': len(text.split()),\n        'sentence_count': len(sent_tokenize(text)),\n        'paragraph_count': len(text.split(\"\\n\")),\n        'stopword_count': len([w for w in text.split() if w.lower() in stop_words]),\n        'unique_word_count': len(set(text.split())),\n        'sentiment_polarity': TextBlob(text).sentiment.polarity,\n        'sentiment_subjectivity': TextBlob(text).sentiment.subjectivity,\n        'discourse_marker_count': count_discourse_markers(text),\n        'vocab_size': vocabSize(text),\n        'sentence_complexity': sentence_complexity(text),\n        'punctuation_count': punctuation_count(text),\n        'sentence_length_difference': sentence_length_difference(text),\n        'type_token_ratio': type_token_ratio(text),\n        'word_entropy': word_entropy(text),\n        'flesch_reading_ease': flesch_reading_ease(text),\n        'gzip_ratio': gzip_ratio(text),\n        'negation_freq': negation_frequency(text),\n        'question_stmt_ratio': question_statement_ratio(text),\n        'clause_sentence_ratio': clause_to_sentence_ratio(text),\n        'modal_freq': modal_verb_frequency(text),\n        'pronoun_ratio': pronoun_ratio(text),\n        'pos_diversity': pos_diversity(text),\n        'hapax_ratio': hapax_legomena_ratio(text),\n        'sentence_length_variation' : sentence_length_variation(text),\n        'repetition_rate' : repetition_rate(text),\n        'personal_voice_score' : personal_voice_score(text),\n        'emotion_variation' : emotion_variation(text),\n        'specificity_score' : specificity_score(text),\n        'figurative_language_score' : figurative_language_score(text),\n        'paragraph_coherence_consistency' : paragraph_coherence_consistency(text),\n        'predictability_score' : predictability_score(text),\n        'hedge_uncertainty_score' : hedge_uncertainty_score(text),\n        'transition_variety_score' : transition_variety_score(text),\n        'grammatical_mistakes':grammatical_mistakes(text),\n        'pos_2gram_variety':pos_ngram_variety(text),\n        'pos_3gram_variety':pos_ngram_variety(text,n=3),\n        'pos_4gram_variety':pos_ngram_variety(text,n=4)\n    }\n    \n    return pd.DataFrame([data])[FEATURES]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:01.684118Z","iopub.execute_input":"2025-12-04T12:53:01.684452Z","iopub.status.idle":"2025-12-04T12:53:03.641819Z","shell.execute_reply.started":"2025-12-04T12:53:01.684424Z","shell.execute_reply":"2025-12-04T12:53:03.640729Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import pandas as pd\nimport shap\nimport numpy as np\n\ndef shap_top_feature_explanations(model, X, sample_features, top_n=10):\n    \"\"\"\n    Return SHAP explanations for ALL features in dictionary form.\n\n    Parameters:\n    - model: trained XGBoost model\n    - X: full training dataframe (feature-only)\n    - sample_features: dataframe containing 1 row of extracted features for the sample\n\n    Returns:\n    - dict of feature → explanation details\n    \"\"\"\n\n    # === SHAP Explainer ===\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer(X)   # background\n    sample_shap = explainer(sample_features)  # current sample\n\n    # Full model-based feature importance\n    importance_df = (\n        pd.DataFrame({\n            \"Feature\": model.get_booster().feature_names,\n            \"Importance\": model.feature_importances_\n        })\n        .sort_values(\"Importance\", ascending=False)\n        .reset_index(drop=True)\n    )\n\n    # Build dictionary of explanations\n    explanations = {}\n\n    for rank, row in importance_df.iterrows():\n        feat = row[\"Feature\"]\n        shap_val = sample_shap.values[0][X.columns.get_loc(feat)]\n        direction = \"AI\" if shap_val > 0 else \"Human\"\n        magnitude = float(abs(shap_val))\n        value = float(sample_features[feat].iloc[0])\n\n        explanations[feat] = {\n            \"Sample_Value\": value,\n            \"SHAP_Value\": float(round(shap_val, 6)),\n            \"Direction\": direction,\n            \"Contribution_Strength\": magnitude,\n            \"Feature_Importance_Rank\": int(rank + 1),\n            \"Model_Importance\": float(row[\"Importance\"])\n        }\n\n    return explanations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:03.643094Z","iopub.execute_input":"2025-12-04T12:53:03.643453Z","iopub.status.idle":"2025-12-04T12:53:03.655484Z","shell.execute_reply.started":"2025-12-04T12:53:03.643425Z","shell.execute_reply":"2025-12-04T12:53:03.654635Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"gpt_text = \"\"\"\nModern classrooms often emphasize technical competence, yet many students still struggle with basic collaboration skills. This challenge becomes visible when group projects break down because individuals feel unheard or undervalued. Teachers often interpret this as a lack of motivation, but the root cause is usually a lack of structured opportunities to practice empathy and communication.\n\nOne effective approach is to encourage students to explain their reasoning rather than defend their conclusions. When a student articulates how they reached an answer, peers begin to see the value in process-oriented thinking. This reduces conflict and fosters a healthier intellectual environment.\n\nAnother strategy involves rotating group roles. When students regularly switch between tasks—such as researcher, scribe, presenter, and evaluator—they develop a broader understanding of how collaborative work functions. It also prevents one individual from dominating the group dynamic.\n\nTeachers can also model productive disagreement. Instead of correcting a student outright, they can pose counterquestions that highlight gaps in reasoning. This shows students that disagreement can lead to clarity rather than confrontation.\n\nRegular reflection sessions amplify these benefits. After each project, students can write short notes about what went well and what created tension. This metacognitive step helps them recognize patterns in their behavior.\n\nImportantly, collaboration skills do not have to be tied only to academic tasks. Shared non-academic activities—such as maintaining a class garden, organizing a small event, or creating artwork—serve as low-pressure environments where students practice cooperation naturally.\n\nClassroom climate plays a major role as well. Students are more willing to collaborate when they feel psychologically safe and respect one another’s contributions. A simple greeting ritual at the beginning of class can set the tone for the entire day.\n\nTechnology can either help or hinder these efforts. Collaborative digital tools make it easier for reserved students to contribute because they have more time to formulate their thoughts. However, overreliance on digital communication reduces face-to-face interaction, which remains essential for building trust.\n\nTeachers should also consider linguistic and cultural diversity within the classroom. A strategy that works well for native speakers may unintentionally exclude multilingual learners. Small adjustments—such as providing sentence starters or allowing more wait time—can make group work significantly more inclusive.\n\nAssessment systems must align with these goals. If grading focuses only on the final product, students will see little value in the collaborative process. Rubrics that reward communication, equitable participation, and constructive feedback create stronger incentives.\n\nFamily engagement enhances collaboration skills outside of school hours. When parents understand the purpose of group activities, they can reinforce these skills at home through shared chores or family discussions.\n\nSchools should also provide professional development for teachers. Many educators were never explicitly taught how to facilitate group dynamics, yet they are expected to manage them effectively. Short workshops can introduce practical tools such as conflict-mapping, turn-taking strategies, and equitable questioning.\n\nFinally, patience is crucial. Collaboration skills are not mastered in a few weeks; they evolve gradually through recurring practice and reflection. Schools that remain committed to the process eventually see improvements not only in group work but also in academic performance and overall classroom culture.\n\nIn the long run, teaching students how to collaborate is as important as teaching them mathematics or reading. These skills shape their relationships, careers, and ability to contribute meaningfully to society.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:03.657064Z","iopub.execute_input":"2025-12-04T12:53:03.657426Z","iopub.status.idle":"2025-12-04T12:53:03.957594Z","shell.execute_reply.started":"2025-12-04T12:53:03.657396Z","shell.execute_reply":"2025-12-04T12:53:03.956758Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"results_df = shap_top_feature_explanations(models['XGB'], X_test, get_features_from_text(gpt_text)[FEATURES])\nprint(\"\\nSummary of Thresholds and SHAP Effects:\\n\", results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:03.958582Z","iopub.execute_input":"2025-12-04T12:53:03.958914Z","iopub.status.idle":"2025-12-04T12:53:15.635691Z","shell.execute_reply.started":"2025-12-04T12:53:03.958882Z","shell.execute_reply":"2025-12-04T12:53:15.634870Z"}},"outputs":[{"name":"stdout","text":"\nSummary of Thresholds and SHAP Effects:\n {'pos_diversity': {'Sample_Value': 2.1033293792915755, 'SHAP_Value': 1.7333519458770752, 'Direction': 'AI', 'Contribution_Strength': 1.7333520650863647, 'Feature_Importance_Rank': 1, 'Model_Importance': 0.44950398802757263}, 'sentence_length_variation': {'Sample_Value': 5.298207802376173, 'SHAP_Value': 1.9536809921264648, 'Direction': 'AI', 'Contribution_Strength': 1.9536807537078857, 'Feature_Importance_Rank': 2, 'Model_Importance': 0.09488911926746368}, 'character_count': {'Sample_Value': 3934.0, 'SHAP_Value': 0.19934800267219543, 'Direction': 'AI', 'Contribution_Strength': 0.19934837520122528, 'Feature_Importance_Rank': 3, 'Model_Importance': 0.03249805420637131}, 'pos_2gram_variety': {'Sample_Value': 75.0, 'SHAP_Value': 0.5615940093994141, 'Direction': 'AI', 'Contribution_Strength': 0.5615938305854797, 'Feature_Importance_Rank': 4, 'Model_Importance': 0.03096909448504448}, 'hapax_ratio': {'Sample_Value': 0.7170418006430869, 'SHAP_Value': 0.5410150289535522, 'Direction': 'AI', 'Contribution_Strength': 0.5410153269767761, 'Feature_Importance_Rank': 5, 'Model_Importance': 0.027856556698679924}, 'flesch_reading_ease': {'Sample_Value': 28.644513387165347, 'SHAP_Value': 1.3171119689941406, 'Direction': 'AI', 'Contribution_Strength': 1.317111611366272, 'Feature_Importance_Rank': 6, 'Model_Importance': 0.027058718726038933}, 'sentence_complexity': {'Sample_Value': 17.16201159458029, 'SHAP_Value': 0.4739609956741333, 'Direction': 'AI', 'Contribution_Strength': 0.4739610552787781, 'Feature_Importance_Rank': 7, 'Model_Importance': 0.025031251832842827}, 'pos_3gram_variety': {'Sample_Value': 227.0, 'SHAP_Value': 0.5369459986686707, 'Direction': 'AI', 'Contribution_Strength': 0.5369461178779602, 'Feature_Importance_Rank': 8, 'Model_Importance': 0.02089882455766201}, 'sentence_length_difference': {'Sample_Value': 21.0, 'SHAP_Value': 1.2591910362243652, 'Direction': 'AI', 'Contribution_Strength': 1.2591907978057861, 'Feature_Importance_Rank': 9, 'Model_Importance': 0.019527804106473923}, 'punctuation_count': {'Sample_Value': 71.0, 'SHAP_Value': -0.38223400712013245, 'Direction': 'Human', 'Contribution_Strength': 0.382234126329422, 'Feature_Importance_Rank': 10, 'Model_Importance': 0.019067076966166496}, 'sentiment_subjectivity': {'Sample_Value': 0.4228734228734229, 'SHAP_Value': -0.10238300263881683, 'Direction': 'Human', 'Contribution_Strength': 0.10238254070281982, 'Feature_Importance_Rank': 11, 'Model_Importance': 0.01847766898572445}, 'grammatical_mistakes': {'Sample_Value': 1.0, 'SHAP_Value': 1.1938159465789795, 'Direction': 'AI', 'Contribution_Strength': 1.193816065788269, 'Feature_Importance_Rank': 12, 'Model_Importance': 0.017863363027572632}, 'specificity_score': {'Sample_Value': 0.27426810477657937, 'SHAP_Value': 0.15914300084114075, 'Direction': 'AI', 'Contribution_Strength': 0.1591426432132721, 'Feature_Importance_Rank': 13, 'Model_Importance': 0.016177702695131302}, 'figurative_language_score': {'Sample_Value': 0.0, 'SHAP_Value': -0.0779390037059784, 'Direction': 'Human', 'Contribution_Strength': 0.07793910056352615, 'Feature_Importance_Rank': 14, 'Model_Importance': 0.016105296090245247}, 'paragraph_count': {'Sample_Value': 29.0, 'SHAP_Value': 0.6076200008392334, 'Direction': 'AI', 'Contribution_Strength': 0.6076195240020752, 'Feature_Importance_Rank': 15, 'Model_Importance': 0.015930717810988426}, 'type_token_ratio': {'Sample_Value': 0.6574585635359116, 'SHAP_Value': 0.4058719873428345, 'Direction': 'AI', 'Contribution_Strength': 0.4058718979358673, 'Feature_Importance_Rank': 16, 'Model_Importance': 0.015076220966875553}, 'stopword_count': {'Sample_Value': 188.0, 'SHAP_Value': 0.9593560099601746, 'Direction': 'AI', 'Contribution_Strength': 0.9593558311462402, 'Feature_Importance_Rank': 17, 'Model_Importance': 0.01441279798746109}, 'pos_4gram_variety': {'Sample_Value': 415.0, 'SHAP_Value': 0.15434199571609497, 'Direction': 'AI', 'Contribution_Strength': 0.15434159338474274, 'Feature_Importance_Rank': 18, 'Model_Importance': 0.014209939166903496}, 'repetition_rate': {'Sample_Value': 0.027675276752767528, 'SHAP_Value': -0.24241000413894653, 'Direction': 'Human', 'Contribution_Strength': 0.24240997433662415, 'Feature_Importance_Rank': 19, 'Model_Importance': 0.01344251073896885}, 'pronoun_ratio': {'Sample_Value': 0.05215827244319654, 'SHAP_Value': -0.8036379814147949, 'Direction': 'Human', 'Contribution_Strength': 0.8036379814147949, 'Feature_Importance_Rank': 20, 'Model_Importance': 0.013381033204495907}, 'gzip_ratio': {'Sample_Value': 0.48347737671581087, 'SHAP_Value': 0.4545080065727234, 'Direction': 'AI', 'Contribution_Strength': 0.45450833439826965, 'Feature_Importance_Rank': 21, 'Model_Importance': 0.013196823187172413}, 'word_entropy': {'Sample_Value': 5.385269661761299, 'SHAP_Value': 1.5706900358200073, 'Direction': 'AI', 'Contribution_Strength': 1.5706894397735596, 'Feature_Importance_Rank': 22, 'Model_Importance': 0.012535622343420982}, 'word_count': {'Sample_Value': 543.0, 'SHAP_Value': -0.08318399637937546, 'Direction': 'Human', 'Contribution_Strength': 0.08318420499563217, 'Feature_Importance_Rank': 23, 'Model_Importance': 0.012351932004094124}, 'transition_variety_score': {'Sample_Value': 2.0, 'SHAP_Value': 0.44912099838256836, 'Direction': 'AI', 'Contribution_Strength': 0.4491214156150818, 'Feature_Importance_Rank': 24, 'Model_Importance': 0.010267123579978943}, 'question_stmt_ratio': {'Sample_Value': 0.0, 'SHAP_Value': -0.17288300395011902, 'Direction': 'Human', 'Contribution_Strength': 0.17288286983966827, 'Feature_Importance_Rank': 25, 'Model_Importance': 0.009542176499962807}, 'paragraph_coherence_consistency': {'Sample_Value': 0.06570761919924752, 'SHAP_Value': -0.5204060077667236, 'Direction': 'Human', 'Contribution_Strength': 0.520405650138855, 'Feature_Importance_Rank': 26, 'Model_Importance': 0.009064534679055214}, 'discourse_marker_count': {'Sample_Value': 1.0, 'SHAP_Value': -0.06450200080871582, 'Direction': 'Human', 'Contribution_Strength': 0.06450244039297104, 'Feature_Importance_Rank': 27, 'Model_Importance': 0.00843796320259571}, 'clause_sentence_ratio': {'Sample_Value': 1.6666666666666667, 'SHAP_Value': -0.38465699553489685, 'Direction': 'Human', 'Contribution_Strength': 0.3846566081047058, 'Feature_Importance_Rank': 28, 'Model_Importance': 0.007846332155168056}, 'modal_freq': {'Sample_Value': 0.02517985566223281, 'SHAP_Value': -0.47422999143600464, 'Direction': 'Human', 'Contribution_Strength': 0.4742303192615509, 'Feature_Importance_Rank': 29, 'Model_Importance': 0.007240788079798222}, 'unique_word_count': {'Sample_Value': 357.0, 'SHAP_Value': 0.01114100031554699, 'Direction': 'AI', 'Contribution_Strength': 0.011141412891447544, 'Feature_Importance_Rank': 30, 'Model_Importance': 0.007138953544199467}}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## Interpretation of results in COLING","metadata":{}},{"cell_type":"code","source":"# results_df = shap_top_feature_explanations(models['XGB'], coling_test_data, get_features_from_text(human_text)[FEATURES])\n# print(\"\\nSummary of Thresholds and SHAP Effects:\\n\", results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.636546Z","iopub.execute_input":"2025-12-04T12:53:15.636833Z","iopub.status.idle":"2025-12-04T12:53:15.640629Z","shell.execute_reply.started":"2025-12-04T12:53:15.636801Z","shell.execute_reply":"2025-12-04T12:53:15.639816Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## Recursive Feature Elimination for PAN CLEF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV, RFE\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.641532Z","iopub.execute_input":"2025-12-04T12:53:15.641784Z","iopub.status.idle":"2025-12-04T12:53:15.685083Z","shell.execute_reply.started":"2025-12-04T12:53:15.641766Z","shell.execute_reply":"2025-12-04T12:53:15.684416Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"\n# cv = StratifiedKFold(\n#     n_splits=5,\n#     shuffle=True,\n#     random_state=42\n# )\n\n# selector = RFECV(\n#     estimator=models['XGB'],\n#     step=1,                  # eliminate one feature at a time\n#     cv=cv,                   # cross-validation method\n#     scoring='f1',       # choose metric\n#     n_jobs=-1\n# )\n\n# selector.fit(X_train, y_train)\n\n# # Selected features\n# selected_features = X_train.columns[selector.support_]\n# print(\"Optimal number of features:\", selector.n_features_)\n# print(\"Selected features:\")\n# print(selected_features)\n\n# # Optional: ranked importance\n# ranking_df = pd.DataFrame({\n#     \"feature\": X_train.columns,\n#     \"rank\": selector.ranking_\n# }).sort_values(\"rank\")\n\n# print(ranking_df)\n\n# ['character_count', 'word_count', 'paragraph_count', 'stopword_count',\n#        'unique_word_count', 'sentiment_subjectivity', 'discourse_marker_count',\n#        'sentence_complexity', 'punctuation_count',\n#        'sentence_length_difference', 'type_token_ratio', 'word_entropy',\n#        'flesch_reading_ease', 'gzip_ratio', 'question_stmt_ratio',\n#        'clause_sentence_ratio', 'modal_freq', 'pronoun_ratio', 'pos_diversity',\n#        'hapax_ratio', 'sentence_length_variation', 'repetition_rate',\n#        'specificity_score', 'figurative_language_score',\n#        'paragraph_coherence_consistency', 'transition_variety_score',\n#        'grammatical_mistakes', 'pos_2gram_variety', 'pos_3gram_variety',\n#        'pos_4gram_variety']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.685894Z","iopub.execute_input":"2025-12-04T12:53:15.686115Z","iopub.status.idle":"2025-12-04T12:53:15.690584Z","shell.execute_reply.started":"2025-12-04T12:53:15.686097Z","shell.execute_reply":"2025-12-04T12:53:15.689868Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Recursive Feature Elimination for COLING","metadata":{}},{"cell_type":"code","source":"# selector = rfe = RFE(\n#     estimator=models['XGB'],\n#     n_features_to_select=19,  # Select top 10 features\n#     step=1  # Remove 1 feature at a time\n# )\n\n# selector.fit(coling_test_data, coling_test_label)\n\n# # Selected features\n# selected_features = coling_test_data.columns[selector.support_]\n# print(\"Optimal number of features:\", selector.n_features_)\n# print(\"Selected features:\")\n# print(selected_features)\n\n# # Optional: ranked importance\n# coling_ranking_df = pd.DataFrame({\n#     \"feature\": coling_test_data.columns,\n#     \"rank\": selector.ranking_\n# }).sort_values(\"rank\")\n\n# print(coling_ranking_df)\n\n# ['character_count', 'word_count', 'sentence_count', 'paragraph_count',\n#        'stopword_count', 'unique_word_count', 'sentiment_subjectivity',\n#        'vocab_size', 'punctuation_count', 'sentence_length_difference',\n#        'type_token_ratio', 'word_entropy', 'flesch_reading_ease', 'gzip_ratio',\n#        'negation_freq', 'question_stmt_ratio', 'clause_sentence_ratio',\n#        'pronoun_ratio', 'hapax_ratio', 'sentence_length_variation',\n#        'repetition_rate', 'personal_voice_score', 'specificity_score',\n#        'paragraph_coherence_consistency', 'predictability_score',\n#        'hedge_uncertainty_score', 'grammatical_mistakes', 'pos_2gram_variety',\n#        'pos_3gram_variety', 'pos_4gram_variety']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.691295Z","iopub.execute_input":"2025-12-04T12:53:15.691643Z","iopub.status.idle":"2025-12-04T12:53:15.708438Z","shell.execute_reply.started":"2025-12-04T12:53:15.691615Z","shell.execute_reply":"2025-12-04T12:53:15.707745Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"preds = [np.array(PAN_preds['Logistic Regression']),np.array(coling_pred['Random Forest'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.709180Z","iopub.execute_input":"2025-12-04T12:53:15.709470Z","iopub.status.idle":"2025-12-04T12:53:15.727335Z","shell.execute_reply.started":"2025-12-04T12:53:15.709451Z","shell.execute_reply":"2025-12-04T12:53:15.726665Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# np.save('optimized_PAN_CLEF_preds_for_COLING.npy',PAN_preds['Logistic Regression'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.728264Z","iopub.execute_input":"2025-12-04T12:53:15.728496Z","iopub.status.idle":"2025-12-04T12:53:15.744273Z","shell.execute_reply.started":"2025-12-04T12:53:15.728478Z","shell.execute_reply":"2025-12-04T12:53:15.743543Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# np.save('optimized_COLING_preds_for_COLING.npy',coling_pred['Random Forest'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.745044Z","iopub.execute_input":"2025-12-04T12:53:15.745237Z","iopub.status.idle":"2025-12-04T12:53:15.758846Z","shell.execute_reply.started":"2025-12-04T12:53:15.745221Z","shell.execute_reply":"2025-12-04T12:53:15.758012Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from scipy.stats import mode\n\noptimized_PAN_CLEF_preds = np.load('/kaggle/working/optimized_COLING_preds_for_COLING.npy')\noptimized_COLING_preds = np.load('/kaggle/working/optimized_PAN_CLEF_preds_for_COLING.npy')\n\npreds = [optimized_PAN_CLEF_preds,optimized_COLING_preds]\n\n# Convert to numpy array for easier manipulation\npreds_array_coling = np.array(preds)\n\n# Get majority prediction - axis=0 calculates mode across models for each sample\nmajority_result_coling = mode(preds_array_coling, axis=0)\n\n# Extract just the majority predictions (ignore the counts)\nmajority_predictions_coling = majority_result_coling.mode\nprint(f\"The f1 score after majority voting is {f1_score(y_test, majority_predictions_coling)}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.856289Z","iopub.status.idle":"2025-12-04T12:53:15.856564Z","shell.execute_reply.started":"2025-12-04T12:53:15.856442Z","shell.execute_reply":"2025-12-04T12:53:15.856453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test set\n# F1 score\n# COLING test set\n# 0.8014803066349458\n# PAN CLEF test set\n# 0.940251572327044\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:53:15.857561Z","iopub.status.idle":"2025-12-04T12:53:15.858071Z","shell.execute_reply.started":"2025-12-04T12:53:15.857942Z","shell.execute_reply":"2025-12-04T12:53:15.857955Z"}},"outputs":[],"execution_count":null}]}