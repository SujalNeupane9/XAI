{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c22c850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:35:56.425308Z",
     "iopub.status.busy": "2025-11-23T06:35:56.425036Z",
     "iopub.status.idle": "2025-11-23T06:36:01.117247Z",
     "shell.execute_reply": "2025-11-23T06:36:01.116010Z"
    },
    "papermill": {
     "duration": 4.700645,
     "end_time": "2025-11-23T06:36:01.119692",
     "exception": false,
     "start_time": "2025-11-23T06:35:56.419047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q /kaggle/input/language-tool-python-2-7-1/language_tool_python-2.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaf765c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:36:01.128508Z",
     "iopub.status.busy": "2025-11-23T06:36:01.128208Z",
     "iopub.status.idle": "2025-11-23T06:36:01.133143Z",
     "shell.execute_reply": "2025-11-23T06:36:01.132427Z"
    },
    "papermill": {
     "duration": 0.010747,
     "end_time": "2025-11-23T06:36:01.134432",
     "exception": false,
     "start_time": "2025-11-23T06:36:01.123685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% Directory settings\n",
    "\n",
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "INPUT_DIR = Path(\"../input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48aa73ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:36:01.142415Z",
     "iopub.status.busy": "2025-11-23T06:36:01.142183Z",
     "iopub.status.idle": "2025-11-23T06:36:01.149725Z",
     "shell.execute_reply": "2025-11-23T06:36:01.148977Z"
    },
    "papermill": {
     "duration": 0.012955,
     "end_time": "2025-11-23T06:36:01.151000",
     "exception": false,
     "start_time": "2025-11-23T06:36:01.138045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/language_tool_python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "\n",
    "# create download path\n",
    "def get_language_tool_cache_path():\n",
    "    # Get download path from environment or use default.\n",
    "    download_path = os.environ.get(\n",
    "        'LTP_PATH',\n",
    "        os.path.join(os.path.expanduser(\"~\"), \".cache\", \"language_tool_python\")\n",
    "    )\n",
    "    # Make download path, if it doesn't exist.\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    return download_path\n",
    "\n",
    "lt_path = get_language_tool_cache_path()\n",
    "lt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4332725e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:36:01.159410Z",
     "iopub.status.busy": "2025-11-23T06:36:01.159150Z",
     "iopub.status.idle": "2025-11-23T06:36:15.485132Z",
     "shell.execute_reply": "2025-11-23T06:36:15.484252Z"
    },
    "papermill": {
     "duration": 14.3321,
     "end_time": "2025-11-23T06:36:15.486676",
     "exception": false,
     "start_time": "2025-11-23T06:36:01.154576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files zipped successfully!\n",
      "Extracted all\n",
      "['LanguageTool-5.7']\n"
     ]
    }
   ],
   "source": [
    "#cant move files directly from input to cache, so we zip it to output and unzip again\n",
    "\n",
    "\n",
    "def get_all_file_paths(directory):\n",
    "  \n",
    "    # initializing empty file paths list\n",
    "    file_paths = []\n",
    "  \n",
    "    # crawling through directory and subdirectories\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)\n",
    "  \n",
    "    # returning all file paths\n",
    "    return file_paths        \n",
    "  \n",
    "def main():\n",
    "    # path to folder which needs to be zipped\n",
    "    directory = '../input/language-tool-python-2-7-1/LanguageTool-5.7/LanguageTool-5.7'\n",
    "  \n",
    "    # calling function to get all file paths in the directory\n",
    "    file_paths = get_all_file_paths(directory)\n",
    "\n",
    "    # writing files to a zipfile\n",
    "    with ZipFile('./lt.zip','w') as zip:\n",
    "        # writing each file one by one\n",
    "        for file in file_paths:\n",
    "            zip.write(file)\n",
    "  \n",
    "    print('All files zipped successfully!')        \n",
    "    \n",
    "main()\n",
    "\n",
    "\n",
    " \n",
    "zip_file = \"./lt.zip\"\n",
    " \n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file) as z:\n",
    "        z.extractall()\n",
    "        print(\"Extracted all\")\n",
    "except:\n",
    "    print(\"Invalid file\")\n",
    "    \n",
    "#move to cache\n",
    "!mv {'./input/language-tool-python-2-7-1/LanguageTool-5.7/LanguageTool-5.7'} {lt_path} \n",
    "print(os.listdir('/root/.cache/language_tool_python/'))\n",
    "\n",
    "#remove files from output\n",
    "\n",
    "shutil.rmtree('./input')\n",
    "os.remove(\"./lt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977bffe6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T06:36:15.495488Z",
     "iopub.status.busy": "2025-11-23T06:36:15.495198Z",
     "iopub.status.idle": "2025-11-23T06:37:00.091538Z",
     "shell.execute_reply": "2025-11-23T06:37:00.090588Z"
    },
    "papermill": {
     "duration": 44.606319,
     "end_time": "2025-11-23T06:37:00.096729",
     "exception": false,
     "start_time": "2025-11-23T06:36:15.490410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7521fa6",
   "metadata": {
    "papermill": {
     "duration": 0.003511,
     "end_time": "2025-11-23T06:37:00.103894",
     "exception": false,
     "start_time": "2025-11-23T06:37:00.100383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5075adb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:37:00.112818Z",
     "iopub.status.busy": "2025-11-23T06:37:00.112230Z",
     "iopub.status.idle": "2025-11-23T06:37:02.899113Z",
     "shell.execute_reply": "2025-11-23T06:37:02.898084Z"
    },
    "papermill": {
     "duration": 2.793284,
     "end_time": "2025-11-23T06:37:02.900782",
     "exception": false,
     "start_time": "2025-11-23T06:37:00.107498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import re\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "import language_tool_python\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def sentence_length_variation(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n",
    "\n",
    "    if len(lengths) < 2:\n",
    "        return 0.0  \n",
    "\n",
    "    return np.std(lengths)   # Standard deviation\n",
    "\n",
    "# def vocabulary_diversity(text): ## same as type token ratio\n",
    "#     words = [w.lower() for w in text.split() if w.isalpha()]\n",
    "#     if len(words) == 0:\n",
    "#         return 0\n",
    "#     return len(set(words)) / len(words)\n",
    "\n",
    "def repetition_rate(text):\n",
    "    words = [w.lower() for w in text.split()]\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    if len(bigrams) == 0:\n",
    "        return 0\n",
    "\n",
    "    counts = Counter(bigrams)\n",
    "    repeated = sum(1 for bg, c in counts.items() if c > 1)\n",
    "\n",
    "    return repeated / len(bigrams)\n",
    "\n",
    "def personal_voice_score(text):\n",
    "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
    "    words = [w.lower() for w in text.split()]\n",
    "    count = sum(1 for w in words if w in personal_pronouns)\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return count / len(words)\n",
    "\n",
    "def emotion_variation(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return 0\n",
    "\n",
    "    sentiments = [TextBlob(s).sentiment.polarity for s in sentences]\n",
    "    diffs = [abs(sentiments[i] - sentiments[i+1]) for i in range(len(sentiments)-1)]\n",
    "\n",
    "    return np.mean(diffs)\n",
    "\n",
    "\n",
    "def specificity_score(text):\n",
    "    doc = nlp(text)\n",
    "    concrete_tags = {\"NOUN\", \"PROPN\", \"NUM\"}  \n",
    "    concrete_count = sum(1 for token in doc if token.pos_ in concrete_tags)\n",
    "    if len(doc) == 0:\n",
    "        return 0\n",
    "    return concrete_count / len(doc)\n",
    "\n",
    "\n",
    "def imperfection_score(text):\n",
    "    words = [w for w in re.findall(r\"\\b\\w+\\b\", text)]\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "\n",
    "    misspelled = sum(1 for w in words if Word(w).correct().lower() != w.lower())\n",
    "    return misspelled / len(words)\n",
    "\n",
    "figurative_markers = [\n",
    "    \"like\", \"as if\", \"as though\", \"metaphor\", \"symbolic\", \n",
    "    \"resembles\", \"reminds me of\", \"figurative\"\n",
    "]\n",
    "\n",
    "def figurative_language_score(text):\n",
    "    t = text.lower()\n",
    "    count = sum(t.count(m) for m in figurative_markers)\n",
    "    return count\n",
    "\n",
    "def paragraph_coherence_consistency(text):\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\") if len(p.strip()) > 0]\n",
    "\n",
    "    if len(paragraphs) < 2:\n",
    "        return 0\n",
    "\n",
    "    vec = TfidfVectorizer().fit_transform(paragraphs)\n",
    "    sims = []\n",
    "\n",
    "    for i in range(len(paragraphs)-1):\n",
    "        sim = cosine_similarity(vec[i], vec[i+1])[0][0]\n",
    "        sims.append(sim)\n",
    "\n",
    "    return np.mean(sims)\n",
    "\n",
    "\n",
    "def predictability_score(text):\n",
    "    words = [w.lower() for w in text.split()]\n",
    "    counts = Counter(words)\n",
    "    total = len(words)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    probs = [counts[w]/total for w in words]\n",
    "    surprise = [-math.log(p) for p in probs]\n",
    "\n",
    "    return np.mean(surprise)\n",
    "\n",
    "hedge_words = {\n",
    "    \"maybe\", \"perhaps\", \"sort of\", \"kind of\", \"i guess\", \n",
    "    \"probably\", \"possibly\", \"apparently\", \"roughly\"\n",
    "}\n",
    "\n",
    "def hedge_uncertainty_score(text):\n",
    "    t = text.lower()\n",
    "    count = sum(t.count(hw) for hw in hedge_words)\n",
    "    return count\n",
    "\n",
    "transitions = [\n",
    "    \"however\", \"therefore\", \"meanwhile\", \"moreover\", \"furthermore\",\n",
    "    \"in contrast\", \"on the other hand\", \"overall\", \"in summary\"\n",
    "]\n",
    "\n",
    "def transition_variety_score(text):\n",
    "    t = text.lower()\n",
    "    count = sum(t.count(word) for word in transitions)\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e47c580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:37:02.909742Z",
     "iopub.status.busy": "2025-11-23T06:37:02.909243Z",
     "iopub.status.idle": "2025-11-23T06:37:05.519180Z",
     "shell.execute_reply": "2025-11-23T06:37:05.518529Z"
    },
    "papermill": {
     "duration": 2.616198,
     "end_time": "2025-11-23T06:37:05.520770",
     "exception": false,
     "start_time": "2025-11-23T06:37:02.904572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/making-features-in-pan-clef-dataset/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/making-features-in-pan-clef-dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ae1e37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T06:37:05.530289Z",
     "iopub.status.busy": "2025-11-23T06:37:05.529864Z",
     "iopub.status.idle": "2025-11-23T07:39:18.290720Z",
     "shell.execute_reply": "2025-11-23T07:39:18.289944Z"
    },
    "papermill": {
     "duration": 3732.767721,
     "end_time": "2025-11-23T07:39:18.292358",
     "exception": false,
     "start_time": "2025-11-23T06:37:05.524637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding sentence_length_variation....\n",
      "Finding repetition_rate....\n",
      "Finding personal_voice_score....\n",
      "Finding emotion_variation....\n",
      "Finding specificity_score....\n",
      "Finding figurative_language_score....\n",
      "Finding paragraph_coherence_consistency....\n",
      "Finding predictability_score....\n",
      "Finding hedge_uncertainty_score....\n",
      "Finding transition_variety_score....\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "print(\"Finding sentence_length_variation....\")\n",
    "train_df['sentence_length_variation'] = train_df['text'].apply(sentence_length_variation)\n",
    "test_df['sentence_length_variation'] = test_df['text'].apply(sentence_length_variation)\n",
    "\n",
    "print(\"Finding repetition_rate....\")\n",
    "train_df['repetition_rate'] = train_df['text'].apply(repetition_rate)\n",
    "test_df['repetition_rate'] = test_df['text'].apply(repetition_rate)\n",
    "\n",
    "print(\"Finding personal_voice_score....\")\n",
    "train_df['personal_voice_score'] = train_df['text'].apply(personal_voice_score)\n",
    "test_df['personal_voice_score'] = test_df['text'].apply(personal_voice_score)\n",
    "\n",
    "print(\"Finding emotion_variation....\")\n",
    "train_df['emotion_variation'] = train_df['text'].apply(emotion_variation)\n",
    "test_df['emotion_variation'] = test_df['text'].apply(emotion_variation)\n",
    "\n",
    "print(\"Finding specificity_score....\")\n",
    "train_df['specificity_score'] = train_df['text'].apply(specificity_score)\n",
    "test_df['specificity_score'] = test_df['text'].apply(specificity_score)\n",
    "\n",
    "# print(\"Finding imperfection_score....\")\n",
    "# train_df['imperfection_score'] = train_df['text'].apply(imperfection_score)\n",
    "# test_df['imperfection_score'] = test_df['text'].apply(imperfection_score)\n",
    "\n",
    "print(\"Finding figurative_language_score....\")\n",
    "train_df['figurative_language_score'] = train_df['text'].apply(figurative_language_score)\n",
    "test_df['figurative_language_score'] = test_df['text'].apply(figurative_language_score)\n",
    "\n",
    "print(\"Finding paragraph_coherence_consistency....\")\n",
    "train_df['paragraph_coherence_consistency'] = train_df['text'].apply(paragraph_coherence_consistency)\n",
    "test_df['paragraph_coherence_consistency'] = test_df['text'].apply(paragraph_coherence_consistency)\n",
    "\n",
    "print(\"Finding predictability_score....\")\n",
    "train_df['predictability_score'] = train_df['text'].apply(predictability_score)\n",
    "test_df['predictability_score'] = test_df['text'].apply(predictability_score)\n",
    "\n",
    "print(\"Finding hedge_uncertainty_score....\")\n",
    "train_df['hedge_uncertainty_score'] = train_df['text'].apply(hedge_uncertainty_score)\n",
    "test_df['hedge_uncertainty_score'] = test_df['text'].apply(hedge_uncertainty_score)\n",
    "\n",
    "print(\"Finding transition_variety_score....\")\n",
    "train_df['transition_variety_score'] = train_df['text'].apply(transition_variety_score)\n",
    "test_df['transition_variety_score'] = test_df['text'].apply(transition_variety_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141bf6c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:39:18.302235Z",
     "iopub.status.busy": "2025-11-23T07:39:18.301984Z",
     "iopub.status.idle": "2025-11-23T07:39:18.306165Z",
     "shell.execute_reply": "2025-11-23T07:39:18.305560Z"
    },
    "papermill": {
     "duration": 0.010529,
     "end_time": "2025-11-23T07:39:18.307302",
     "exception": false,
     "start_time": "2025-11-23T07:39:18.296773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grammatical_mistakes(sentence):\n",
    "\n",
    "  mistakes = len(tool.check(sentence))\n",
    "\n",
    "  return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be830be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:39:18.316186Z",
     "iopub.status.busy": "2025-11-23T07:39:18.315944Z",
     "iopub.status.idle": "2025-11-23T10:11:10.383174Z",
     "shell.execute_reply": "2025-11-23T10:11:10.381910Z"
    },
    "papermill": {
     "duration": 9112.074243,
     "end_time": "2025-11-23T10:11:10.385487",
     "exception": false,
     "start_time": "2025-11-23T07:39:18.311244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['grammatical_mistakes'] = train_df['text'].apply(grammatical_mistakes)\n",
    "test_df['grammatical_mistakes'] = test_df['text'].apply(grammatical_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32cdb70b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T10:11:10.397760Z",
     "iopub.status.busy": "2025-11-23T10:11:10.396833Z",
     "iopub.status.idle": "2025-11-23T10:11:10.404522Z",
     "shell.execute_reply": "2025-11-23T10:11:10.403892Z"
    },
    "papermill": {
     "duration": 0.014364,
     "end_time": "2025-11-23T10:11:10.405691",
     "exception": false,
     "start_time": "2025-11-23T10:11:10.391327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_tag_ngrams(text, n=2):\n",
    "    doc = nlp(text)\n",
    "    tags = [token.pos_ for token in doc if token.is_alpha]\n",
    "\n",
    "    if len(tags) < n:\n",
    "        return {}\n",
    "\n",
    "    ngrams = zip(*[tags[i:] for i in range(n)])\n",
    "    return Counter(ngrams)\n",
    "\n",
    "def pos_ngram_variety(text, n=2):\n",
    "    ngrams = pos_tag_ngrams(text, n)\n",
    "    return len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cea8b8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T10:11:10.416557Z",
     "iopub.status.busy": "2025-11-23T10:11:10.416299Z",
     "iopub.status.idle": "2025-11-23T12:54:19.478053Z",
     "shell.execute_reply": "2025-11-23T12:54:19.476809Z"
    },
    "papermill": {
     "duration": 9789.068789,
     "end_time": "2025-11-23T12:54:19.480242",
     "exception": false,
     "start_time": "2025-11-23T10:11:10.411453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['pos_2gram_variety'] = train_df['text'].apply(pos_ngram_variety)\n",
    "test_df['pos_2gram_variety'] = test_df['text'].apply(pos_ngram_variety)\n",
    "## number of 3 grams\n",
    "train_df['pos_3gram_variety'] = train_df['text'].apply(lambda x: pos_ngram_variety(x, n=3))\n",
    "test_df['pos_3gram_variety'] = test_df['text'].apply(lambda x: pos_ngram_variety(x, n=3))\n",
    "## number of 4 grams\n",
    "train_df['pos_4gram_variety'] = train_df['text'].apply(lambda x: pos_ngram_variety(x, n=4))\n",
    "test_df['pos_4gram_variety'] = test_df['text'].apply(lambda x: pos_ngram_variety(x, n=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14813bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T12:54:19.491596Z",
     "iopub.status.busy": "2025-11-23T12:54:19.491322Z",
     "iopub.status.idle": "2025-11-23T12:54:19.496469Z",
     "shell.execute_reply": "2025-11-23T12:54:19.495787Z"
    },
    "papermill": {
     "duration": 0.011864,
     "end_time": "2025-11-23T12:54:19.497606",
     "exception": false,
     "start_time": "2025-11-23T12:54:19.485742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "# # Load once (place globally)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"/kaggle/input/gpt2/transformers/gpt21/1\")\n",
    "# gpt_model = GPT2LMHeadModel.from_pretrained(\"/kaggle/input/gpt2/transformers/gpt21/1\")\n",
    "# gpt_model.eval()\n",
    "\n",
    "# def perplexity(text):\n",
    "#     enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "#     input_ids = enc.input_ids\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = gpt_model(input_ids, labels=input_ids)\n",
    "\n",
    "#     loss = outputs.loss\n",
    "#     return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a077e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T12:54:19.507017Z",
     "iopub.status.busy": "2025-11-23T12:54:19.506767Z",
     "iopub.status.idle": "2025-11-23T12:54:19.510138Z",
     "shell.execute_reply": "2025-11-23T12:54:19.509500Z"
    },
    "papermill": {
     "duration": 0.009594,
     "end_time": "2025-11-23T12:54:19.511304",
     "exception": false,
     "start_time": "2025-11-23T12:54:19.501710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df['perplexity'] = train_df['text'].apply(perplexity)\n",
    "# test_df['perplexity'] = test_df['text'].apply(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d210fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T12:54:19.520550Z",
     "iopub.status.busy": "2025-11-23T12:54:19.520298Z",
     "iopub.status.idle": "2025-11-23T12:54:24.011527Z",
     "shell.execute_reply": "2025-11-23T12:54:24.010827Z"
    },
    "papermill": {
     "duration": 4.49765,
     "end_time": "2025-11-23T12:54:24.013030",
     "exception": false,
     "start_time": "2025-11-23T12:54:19.515380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv',index=False)\n",
    "test_df.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0199e",
   "metadata": {
    "papermill": {
     "duration": 0.004181,
     "end_time": "2025-11-23T12:54:24.021411",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.017230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684b81a",
   "metadata": {
    "papermill": {
     "duration": 0.004066,
     "end_time": "2025-11-23T12:54:24.029504",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.025438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4be144",
   "metadata": {
    "papermill": {
     "duration": 0.003908,
     "end_time": "2025-11-23T12:54:24.037060",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.033152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b00be1",
   "metadata": {
    "papermill": {
     "duration": 0.003728,
     "end_time": "2025-11-23T12:54:24.044550",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.040822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbafaea",
   "metadata": {
    "papermill": {
     "duration": 0.00396,
     "end_time": "2025-11-23T12:54:24.052485",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.048525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d192ed",
   "metadata": {
    "papermill": {
     "duration": 0.003987,
     "end_time": "2025-11-23T12:54:24.060486",
     "exception": false,
     "start_time": "2025-11-23T12:54:24.056499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3968241,
     "sourceId": 6909860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6849972,
     "sourceId": 11003597,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 272003408,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 91833,
     "modelInstanceId": 66816,
     "sourceId": 79529,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22714.419895,
   "end_time": "2025-11-23T12:54:26.886009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T06:35:52.466114",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
