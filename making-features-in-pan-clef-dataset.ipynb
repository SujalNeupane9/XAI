{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75458309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:03.390742Z",
     "iopub.status.busy": "2025-10-30T05:11:03.390477Z",
     "iopub.status.idle": "2025-10-30T05:11:38.172827Z",
     "shell.execute_reply": "2025-10-30T05:11:38.172019Z"
    },
    "papermill": {
     "duration": 34.796281,
     "end_time": "2025-10-30T05:11:38.179647",
     "exception": false,
     "start_time": "2025-10-30T05:11:03.383366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "import scipy.sparse as sp\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff69a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:38.191229Z",
     "iopub.status.busy": "2025-10-30T05:11:38.190553Z",
     "iopub.status.idle": "2025-10-30T05:11:42.860223Z",
     "shell.execute_reply": "2025-10-30T05:11:42.858817Z"
    },
    "papermill": {
     "duration": 4.677353,
     "end_time": "2025-10-30T05:11:42.862128",
     "exception": false,
     "start_time": "2025-10-30T05:11:38.184775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install textstat -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250b8558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:42.874036Z",
     "iopub.status.busy": "2025-10-30T05:11:42.873664Z",
     "iopub.status.idle": "2025-10-30T05:11:47.027309Z",
     "shell.execute_reply": "2025-10-30T05:11:47.026194Z"
    },
    "papermill": {
     "duration": 4.161408,
     "end_time": "2025-10-30T05:11:47.028920",
     "exception": false,
     "start_time": "2025-10-30T05:11:42.867512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install language-tool-python -q\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca811069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:47.041674Z",
     "iopub.status.busy": "2025-10-30T05:11:47.041364Z",
     "iopub.status.idle": "2025-10-30T05:11:47.207365Z",
     "shell.execute_reply": "2025-10-30T05:11:47.206357Z"
    },
    "papermill": {
     "duration": 0.174422,
     "end_time": "2025-10-30T05:11:47.209087",
     "exception": false,
     "start_time": "2025-10-30T05:11:47.034665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "import textstat\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US', remote_server='https://api.languagetool.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c2aa39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:47.221135Z",
     "iopub.status.busy": "2025-10-30T05:11:47.220755Z",
     "iopub.status.idle": "2025-10-30T05:11:49.751323Z",
     "shell.execute_reply": "2025-10-30T05:11:49.750259Z"
    },
    "papermill": {
     "duration": 2.538546,
     "end_time": "2025-10-30T05:11:49.753100",
     "exception": false,
     "start_time": "2025-10-30T05:11:47.214554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/pan-cief/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/pan-cief/val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93248c79",
   "metadata": {
    "papermill": {
     "duration": 0.005158,
     "end_time": "2025-10-30T05:11:49.763710",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.758552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ce3e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.775657Z",
     "iopub.status.busy": "2025-10-30T05:11:49.775070Z",
     "iopub.status.idle": "2025-10-30T05:11:49.780004Z",
     "shell.execute_reply": "2025-10-30T05:11:49.779212Z"
    },
    "papermill": {
     "duration": 0.012433,
     "end_time": "2025-10-30T05:11:49.781318",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.768885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vocabSize(sentence):\n",
    "\n",
    "    doc = nlp(sentence.lower())\n",
    "    tokens = set([token.text for token in doc if not token.is_punct])\n",
    "    vocab_size = len(tokens)\n",
    "\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0477588f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.793588Z",
     "iopub.status.busy": "2025-10-30T05:11:49.792736Z",
     "iopub.status.idle": "2025-10-30T05:11:49.798026Z",
     "shell.execute_reply": "2025-10-30T05:11:49.797265Z"
    },
    "papermill": {
     "duration": 0.012593,
     "end_time": "2025-10-30T05:11:49.799361",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.786768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_complexity(sentence):\n",
    "\n",
    "  flesch_score = textstat.flesch_reading_ease(sentence) # Flesch Reading Ease: Measures the readability of a sentence. Higher scores indicate easier readability.\n",
    "  fk_grade_level = textstat.flesch_kincaid_grade(sentence) # Flesch-Kincaid Grade Level: Indicates the grade level required to understand the text. Higher scores mean more complex text.\n",
    "  gunning_fog = textstat.gunning_fog(sentence) # Gunning Fog Index: Measures the readability of the text, considering sentence length and complex words (3+ syllables). Higher values indicate more complex text.\n",
    "  smog_index = textstat.smog_index(sentence)  # SMOG Index: Estimates the years of education required to understand the text. Higher values indicate more complex text.\n",
    "  composite_score = (flesch_score * 0.2 + fk_grade_level * 0.3 + gunning_fog * 0.3 + smog_index * 0.2)\n",
    "    # print(f\"Composite Sentence Complexity Score(In scale of 1 - 20): {composite_score}\")\n",
    "\n",
    "  return composite_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafa6ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.811710Z",
     "iopub.status.busy": "2025-10-30T05:11:49.811399Z",
     "iopub.status.idle": "2025-10-30T05:11:49.815599Z",
     "shell.execute_reply": "2025-10-30T05:11:49.814822Z"
    },
    "papermill": {
     "duration": 0.012227,
     "end_time": "2025-10-30T05:11:49.817254",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.805027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grammatical_mistakes(sentence):\n",
    "\n",
    "  mistakes = len(tool.check(sentence))\n",
    "\n",
    "  return mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c50cc5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.829366Z",
     "iopub.status.busy": "2025-10-30T05:11:49.829072Z",
     "iopub.status.idle": "2025-10-30T05:11:49.834351Z",
     "shell.execute_reply": "2025-10-30T05:11:49.833362Z"
    },
    "papermill": {
     "duration": 0.013119,
     "end_time": "2025-10-30T05:11:49.835860",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.822741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def punctuation_count(paragraph):\n",
    "    # Create a dictionary to store counts of each punctuation mark\n",
    "    punctuation_counts = {p: 0 for p in string.punctuation}\n",
    "\n",
    "    # Count each punctuation mark in the paragraph\n",
    "    for char in paragraph:\n",
    "        if char in string.punctuation:\n",
    "            punctuation_counts[char] += 1\n",
    "\n",
    "    # Calculate the total number of punctuation marks\n",
    "    total_punctuation = sum(punctuation_counts.values())\n",
    "\n",
    "    return total_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702fda93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.847616Z",
     "iopub.status.busy": "2025-10-30T05:11:49.847341Z",
     "iopub.status.idle": "2025-10-30T05:11:49.853055Z",
     "shell.execute_reply": "2025-10-30T05:11:49.852146Z"
    },
    "papermill": {
     "duration": 0.013289,
     "end_time": "2025-10-30T05:11:49.854497",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.841208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentence_length_difference(paragraph):\n",
    "\n",
    "    # Split the paragraph into sentences using common sentence delimiters\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', paragraph)\n",
    "\n",
    "    # Remove empty strings and whitespace from the list of sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    # Calculate the length of each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Calculate the difference between the smallest and largest sentence length\n",
    "    length_difference = max(sentence_lengths) - min(sentence_lengths) if sentence_lengths else 0\n",
    "\n",
    "    return length_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0092ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.866292Z",
     "iopub.status.busy": "2025-10-30T05:11:49.865966Z",
     "iopub.status.idle": "2025-10-30T05:11:49.870891Z",
     "shell.execute_reply": "2025-10-30T05:11:49.870106Z"
    },
    "papermill": {
     "duration": 0.012414,
     "end_time": "2025-10-30T05:11:49.872466",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.860052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def type_token_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculates the Type-Token Ratio (TTR) for vocabulary diversity.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words) if len(words) > 0 else 0\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6316179e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.886024Z",
     "iopub.status.busy": "2025-10-30T05:11:49.885730Z",
     "iopub.status.idle": "2025-10-30T05:11:49.915169Z",
     "shell.execute_reply": "2025-10-30T05:11:49.914396Z"
    },
    "papermill": {
     "duration": 0.03889,
     "end_time": "2025-10-30T05:11:49.916794",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.877904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['character_count'] = train_df['text'].str.len()\n",
    "test_df['character_count'] = test_df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5bb9aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:49.930483Z",
     "iopub.status.busy": "2025-10-30T05:11:49.930224Z",
     "iopub.status.idle": "2025-10-30T05:11:50.987104Z",
     "shell.execute_reply": "2025-10-30T05:11:50.985892Z"
    },
    "papermill": {
     "duration": 1.064443,
     "end_time": "2025-10-30T05:11:50.988727",
     "exception": false,
     "start_time": "2025-10-30T05:11:49.924284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50254e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:51.001276Z",
     "iopub.status.busy": "2025-10-30T05:11:51.000918Z",
     "iopub.status.idle": "2025-10-30T05:11:52.126385Z",
     "shell.execute_reply": "2025-10-30T05:11:52.125361Z"
    },
    "papermill": {
     "duration": 1.133628,
     "end_time": "2025-10-30T05:11:52.127881",
     "exception": false,
     "start_time": "2025-10-30T05:11:50.994253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['sentence_count'] = train_df['text'].apply(lambda x: len(re.split(r'[.!?]', x.strip())) - 1)\n",
    "test_df['sentence_count'] = test_df['text'].apply(lambda x: len(re.split(r'[.!?]', x.strip())) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6cc0826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:52.143193Z",
     "iopub.status.busy": "2025-10-30T05:11:52.142229Z",
     "iopub.status.idle": "2025-10-30T05:11:52.283273Z",
     "shell.execute_reply": "2025-10-30T05:11:52.282244Z"
    },
    "papermill": {
     "duration": 0.150957,
     "end_time": "2025-10-30T05:11:52.284845",
     "exception": false,
     "start_time": "2025-10-30T05:11:52.133888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['paragraph_count'] = train_df['text'].apply(lambda x: len(x.split(\"\\n\")))\n",
    "test_df['paragraph_count'] = test_df['text'].apply(lambda x: len(x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c89b5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:52.297281Z",
     "iopub.status.busy": "2025-10-30T05:11:52.296470Z",
     "iopub.status.idle": "2025-10-30T05:11:55.561521Z",
     "shell.execute_reply": "2025-10-30T05:11:55.560664Z"
    },
    "papermill": {
     "duration": 3.272818,
     "end_time": "2025-10-30T05:11:55.563240",
     "exception": false,
     "start_time": "2025-10-30T05:11:52.290422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['stopword_count'] = train_df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "test_df['stopword_count'] = test_df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88964eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:55.575077Z",
     "iopub.status.busy": "2025-10-30T05:11:55.574753Z",
     "iopub.status.idle": "2025-10-30T05:11:57.904698Z",
     "shell.execute_reply": "2025-10-30T05:11:57.903642Z"
    },
    "papermill": {
     "duration": 2.337677,
     "end_time": "2025-10-30T05:11:57.906417",
     "exception": false,
     "start_time": "2025-10-30T05:11:55.568740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(x.split())))\n",
    "test_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f5e4c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T05:11:57.918752Z",
     "iopub.status.busy": "2025-10-30T05:11:57.918430Z",
     "iopub.status.idle": "2025-10-30T06:08:04.447349Z",
     "shell.execute_reply": "2025-10-30T06:08:04.446130Z"
    },
    "papermill": {
     "duration": 3366.537887,
     "end_time": "2025-10-30T06:08:04.449909",
     "exception": false,
     "start_time": "2025-10-30T05:11:57.912022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    pos_count_dict = {}\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_count_dict[pos] = pos_count_dict.get(pos, 0) + 1\n",
    "    return pos_count_dict\n",
    "\n",
    "train_df['pos_counts'] = train_df['text'].apply(pos_counts)\n",
    "test_df['pos_counts'] = test_df['text'].apply(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be92a5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:08:04.464232Z",
     "iopub.status.busy": "2025-10-30T06:08:04.463828Z",
     "iopub.status.idle": "2025-10-30T06:11:05.237753Z",
     "shell.execute_reply": "2025-10-30T06:11:05.236704Z"
    },
    "papermill": {
     "duration": 180.782397,
     "end_time": "2025-10-30T06:11:05.239650",
     "exception": false,
     "start_time": "2025-10-30T06:08:04.457253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "train_df['sentiment_polarity'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "train_df['sentiment_subjectivity'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "test_df['sentiment_polarity'] = test_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "test_df['sentiment_subjectivity'] = test_df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e715a41b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:11:05.252076Z",
     "iopub.status.busy": "2025-10-30T06:11:05.251753Z",
     "iopub.status.idle": "2025-10-30T06:11:08.665573Z",
     "shell.execute_reply": "2025-10-30T06:11:08.664358Z"
    },
    "papermill": {
     "duration": 3.421867,
     "end_time": "2025-10-30T06:11:08.667363",
     "exception": false,
     "start_time": "2025-10-30T06:11:05.245496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discourse_markers = [\"however\", \"therefore\", \"although\", \"nevertheless\", \"hence\"]\n",
    "\n",
    "def count_discourse_markers(text):\n",
    "    return sum(text.lower().count(marker) for marker in discourse_markers)\n",
    "\n",
    "train_df['discourse_marker_count'] = train_df['text'].apply(count_discourse_markers)\n",
    "test_df['discourse_marker_count'] = test_df['text'].apply(count_discourse_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c82f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:11:08.679491Z",
     "iopub.status.busy": "2025-10-30T06:11:08.678560Z",
     "iopub.status.idle": "2025-10-30T07:01:18.094874Z",
     "shell.execute_reply": "2025-10-30T07:01:18.093877Z"
    },
    "papermill": {
     "duration": 3009.424101,
     "end_time": "2025-10-30T07:01:18.096723",
     "exception": false,
     "start_time": "2025-10-30T06:11:08.672622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['vocab_size'] = train_df['text'].apply(vocabSize)\n",
    "train_df['sentence_complexity'] = train_df['text'].apply(sentence_complexity)\n",
    "# train_df['grammatical_mistakes'] = train_df['text'].apply(grammatical_mistakes)\n",
    "train_df['punctuation_count'] = train_df['text'].apply(punctuation_count)\n",
    "train_df['sentence_length_difference'] = train_df['text'].apply(sentence_length_difference)\n",
    "train_df['type_token_ratio'] = train_df['text'].apply(type_token_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3bef5ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:01:18.109033Z",
     "iopub.status.busy": "2025-10-30T07:01:18.108726Z",
     "iopub.status.idle": "2025-10-30T07:08:45.580346Z",
     "shell.execute_reply": "2025-10-30T07:08:45.579525Z"
    },
    "papermill": {
     "duration": 447.479866,
     "end_time": "2025-10-30T07:08:45.582284",
     "exception": false,
     "start_time": "2025-10-30T07:01:18.102418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['vocab_size'] = test_df['text'].apply(vocabSize)\n",
    "test_df['sentence_complexity'] = test_df['text'].apply(sentence_complexity)\n",
    "# test_df['grammatical_mistakes'] = test_df['text'].apply(grammatical_mistakes)\n",
    "test_df['punctuation_count'] = test_df['text'].apply(punctuation_count)\n",
    "test_df['sentence_length_difference'] = test_df['text'].apply(sentence_length_difference)\n",
    "test_df['type_token_ratio'] = test_df['text'].apply(type_token_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa7783e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:08:45.594646Z",
     "iopub.status.busy": "2025-10-30T07:08:45.594342Z",
     "iopub.status.idle": "2025-10-30T07:08:45.600042Z",
     "shell.execute_reply": "2025-10-30T07:08:45.599463Z"
    },
    "papermill": {
     "duration": 0.012912,
     "end_time": "2025-10-30T07:08:45.601258",
     "exception": false,
     "start_time": "2025-10-30T07:08:45.588346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def word_entropy(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.lemma_.lower() for t in doc if t.is_alpha]\n",
    "    freqs = list(Counter(words).values())\n",
    "    return entropy(freqs) if freqs else 0\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    try:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def gzip_ratio(text):\n",
    "    import gzip\n",
    "    compressed = len(gzip.compress(text.encode('utf-8')))\n",
    "    return compressed / len(text) if len(text) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "616fdc1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:08:45.612804Z",
     "iopub.status.busy": "2025-10-30T07:08:45.612517Z",
     "iopub.status.idle": "2025-10-30T07:57:47.389515Z",
     "shell.execute_reply": "2025-10-30T07:57:47.388660Z"
    },
    "papermill": {
     "duration": 2941.784735,
     "end_time": "2025-10-30T07:57:47.391266",
     "exception": false,
     "start_time": "2025-10-30T07:08:45.606531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['word_entropy'] = train_df['text'].apply(word_entropy)\n",
    "train_df['flesch_reading_ease'] = train_df['text'].apply(flesch_reading_ease)\n",
    "train_df['gzip_ratio'] = train_df['text'].apply(gzip_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f8821c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T07:57:47.403655Z",
     "iopub.status.busy": "2025-10-30T07:57:47.403290Z",
     "iopub.status.idle": "2025-10-30T08:05:03.259089Z",
     "shell.execute_reply": "2025-10-30T08:05:03.258285Z"
    },
    "papermill": {
     "duration": 435.863702,
     "end_time": "2025-10-30T08:05:03.260786",
     "exception": false,
     "start_time": "2025-10-30T07:57:47.397084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['word_entropy'] = test_df['text'].apply(word_entropy)\n",
    "test_df['flesch_reading_ease'] = test_df['text'].apply(flesch_reading_ease)\n",
    "test_df['gzip_ratio'] = test_df['text'].apply(gzip_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ba28a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:05:03.273719Z",
     "iopub.status.busy": "2025-10-30T08:05:03.272919Z",
     "iopub.status.idle": "2025-10-30T08:05:03.286588Z",
     "shell.execute_reply": "2025-10-30T08:05:03.285542Z"
    },
    "papermill": {
     "duration": 0.022565,
     "end_time": "2025-10-30T08:05:03.289127",
     "exception": false,
     "start_time": "2025-10-30T08:05:03.266562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negation_frequency(text):\n",
    "    doc = nlp(text)\n",
    "    negations = [t for t in doc if t.dep_ == \"neg\" or t.lemma_.lower() in [\"not\", \"no\", \"never\", \"none\", \"n't\"]]\n",
    "    total_words = len([t for t in doc if t.is_alpha])\n",
    "    return len(negations) / (total_words + 1e-5)\n",
    "\n",
    "def question_statement_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    question_count = sum(1 for s in sentences if s.text.strip().endswith(\"?\"))\n",
    "    statement_count = sum(1 for s in sentences if s.text.strip().endswith(\".\"))\n",
    "    return question_count / (statement_count + 1e-5)\n",
    "\n",
    "def clause_to_sentence_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    clause_markers = (\"mark\", \"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\", \"conj\")\n",
    "    clause_count = sum(1 for t in doc if t.dep_ in clause_markers)\n",
    "    return clause_count / len(sentences)\n",
    "\n",
    "def modal_verb_frequency(text):\n",
    "    doc = nlp(text)\n",
    "    modals = {\"can\", \"could\", \"may\", \"might\", \"shall\", \"should\", \"will\", \"would\", \"must\"}\n",
    "    modal_count = sum(1 for t in doc if t.lemma_.lower() in modals)\n",
    "    total_words = len([t for t in doc if t.is_alpha])\n",
    "    return modal_count / (total_words + 1e-5)\n",
    "\n",
    "def pronoun_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    pronouns = [t for t in doc if t.pos_ == \"PRON\"]\n",
    "    total_words = len([t for t in doc if t.is_alpha])\n",
    "    return len(pronouns) / (total_words + 1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35763a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:05:03.303131Z",
     "iopub.status.busy": "2025-10-30T08:05:03.302724Z",
     "iopub.status.idle": "2025-10-30T08:05:03.312863Z",
     "shell.execute_reply": "2025-10-30T08:05:03.312160Z"
    },
    "papermill": {
     "duration": 0.018188,
     "end_time": "2025-10-30T08:05:03.314330",
     "exception": false,
     "start_time": "2025-10-30T08:05:03.296142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def pos_diversity(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [t.pos_ for t in doc if t.is_alpha]\n",
    "    counts = Counter(pos_tags)\n",
    "    return entropy(list(counts.values())) if counts else 0\n",
    "\n",
    "def pos_ngram_frequency_deviation(text, ref_pos_ngrams=None, n=2):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [t.pos_ for t in doc if t.is_alpha]\n",
    "    ngrams = [\"_\".join(pos_tags[i:i+n]) for i in range(len(pos_tags)-n+1)]\n",
    "    counts = Counter(ngrams)\n",
    "    total = sum(counts.values())\n",
    "    if total == 0 or ref_pos_ngrams is None:\n",
    "        return 0\n",
    "    # Normalize\n",
    "    probs = {k: v/total for k, v in counts.items()}\n",
    "    # Compute chi-square-like deviation\n",
    "    deviation = 0\n",
    "    for k, p in probs.items():\n",
    "        expected = ref_pos_ngrams.get(k, 1e-6)\n",
    "        deviation += ((p - expected) ** 2) / expected\n",
    "    return deviation\n",
    "\n",
    "def hapax_legomena_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    words = [t.lemma_.lower() for t in doc if t.is_alpha]\n",
    "    if not words:\n",
    "        return 0\n",
    "    freqs = Counter(words)\n",
    "    hapax = sum(1 for w, f in freqs.items() if f == 1)\n",
    "    return hapax / len(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f06cf96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T08:05:03.327259Z",
     "iopub.status.busy": "2025-10-30T08:05:03.326527Z",
     "iopub.status.idle": "2025-10-30T13:46:58.819501Z",
     "shell.execute_reply": "2025-10-30T13:46:58.818483Z"
    },
    "papermill": {
     "duration": 20515.501752,
     "end_time": "2025-10-30T13:46:58.821652",
     "exception": false,
     "start_time": "2025-10-30T08:05:03.319900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['negation_freq'] = train_df['text'].apply(negation_frequency)\n",
    "train_df['question_stmt_ratio'] = train_df['text'].apply(question_statement_ratio)\n",
    "train_df['clause_sentence_ratio'] = train_df['text'].apply(clause_to_sentence_ratio)\n",
    "train_df['modal_freq'] = train_df['text'].apply(modal_verb_frequency)\n",
    "train_df['pronoun_ratio'] = train_df['text'].apply(pronoun_ratio)\n",
    "train_df['pos_diversity'] = train_df['text'].apply(pos_diversity)\n",
    "train_df['hapax_ratio'] = train_df['text'].apply(hapax_legomena_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df02a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T13:46:58.836279Z",
     "iopub.status.busy": "2025-10-30T13:46:58.835491Z",
     "iopub.status.idle": "2025-10-30T14:37:42.081118Z",
     "shell.execute_reply": "2025-10-30T14:37:42.080223Z"
    },
    "papermill": {
     "duration": 3043.254058,
     "end_time": "2025-10-30T14:37:42.083262",
     "exception": false,
     "start_time": "2025-10-30T13:46:58.829204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['negation_freq'] = test_df['text'].apply(negation_frequency)\n",
    "test_df['question_stmt_ratio'] = test_df['text'].apply(question_statement_ratio)\n",
    "test_df['clause_sentence_ratio'] = test_df['text'].apply(clause_to_sentence_ratio)\n",
    "test_df['modal_freq'] = test_df['text'].apply(modal_verb_frequency)\n",
    "test_df['pronoun_ratio'] = test_df['text'].apply(pronoun_ratio)\n",
    "test_df['pos_diversity'] = test_df['text'].apply(pos_diversity)\n",
    "test_df['hapax_ratio'] = test_df['text'].apply(hapax_legomena_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ea95d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:37:42.096907Z",
     "iopub.status.busy": "2025-10-30T14:37:42.096614Z",
     "iopub.status.idle": "2025-10-30T14:37:45.945372Z",
     "shell.execute_reply": "2025-10-30T14:37:45.944406Z"
    },
    "papermill": {
     "duration": 3.857289,
     "end_time": "2025-10-30T14:37:45.947041",
     "exception": false,
     "start_time": "2025-10-30T14:37:42.089752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e623d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T14:37:45.960214Z",
     "iopub.status.busy": "2025-10-30T14:37:45.959579Z",
     "iopub.status.idle": "2025-10-30T14:37:46.546676Z",
     "shell.execute_reply": "2025-10-30T14:37:46.545648Z"
    },
    "papermill": {
     "duration": 0.595493,
     "end_time": "2025-10-30T14:37:46.548412",
     "exception": false,
     "start_time": "2025-10-30T14:37:45.952919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6849972,
     "sourceId": 11003597,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34010.18233,
   "end_time": "2025-10-30T14:37:49.274780",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-30T05:10:59.092450",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
